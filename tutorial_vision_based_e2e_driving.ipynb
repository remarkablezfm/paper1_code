{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBFvXsCOxHeV"
      },
      "source": [
        "#  Vision-based End-to-End Driving Tutorial\n",
        "\n",
        "- Website: https://waymo.com/open\n",
        "- GitHub: https://github.com/waymo-research/waymo-open-dataset\n",
        "- Challenge: https://waymo.com/open/challenges/2025/e2e-driving/\n",
        "\n",
        "This tutorial demonstrates how to load, visualize and submit end-to-end driving data. Visit the [Waymo Open Dataset Website](https://waymo.com/open) to download the full dataset.\n",
        "\n",
        "To use, open this notebook in [Colab](https://colab.research.google.com).\n",
        "\n",
        "Uncheck the box \"Reset all runtimes before running\" if you run this colab directly from the remote kernel. Alternatively, you can make a copy before trying to run it by following \"File > Save copy in Drive ...\".\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBYit3Qhxw3E"
      },
      "source": [
        "## Package installation ğŸ› ï¸"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vBAbHAxuwmic"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-01-05 15:20:34.026546: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2026-01-05 15:20:34.029401: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2026-01-05 15:20:34.067063: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2026-01-05 15:20:34.067618: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2026-01-05 15:20:34.469882: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "from typing import Tuple\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "import cv2\n",
        "from waymo_open_dataset import dataset_pb2 as open_dataset\n",
        "from waymo_open_dataset.wdl_limited.camera.ops import py_camera_model_ops\n",
        "\n",
        "from waymo_open_dataset.protos import end_to_end_driving_data_pb2 as wod_e2ed_pb2\n",
        "from waymo_open_dataset.protos import end_to_end_driving_submission_pb2 as wod_e2ed_submission_pb2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLqKE2xj-tHH"
      },
      "source": [
        "## Loading the data\n",
        "\n",
        "Visit the [Waymo Open Dataset Website](https://waymo.com/open/) to download the\n",
        "full dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“‚ æ­£åœ¨æ£€æŸ¥ç›®å½•: /mnt/d/Datasets/WOD_E2E_Camera_v1/val\n",
            "ğŸ“„ å®é™…æ–‡ä»¶åˆ—è¡¨ (å‰3ä¸ª): ['val_202504211843.tfrecord-00000-of-00093', 'val_202504211843.tfrecord-00001-of-00093', 'val_202504211843.tfrecord-00002-of-00093']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "DATASET_FOLDER = '/mnt/d/Datasets/WOD_E2E_Camera_v1/val'\n",
        "\n",
        "# æ‰“å°æ–‡ä»¶å¤¹é‡Œå‰ 3 ä¸ªæ–‡ä»¶çœ‹çœ‹åå­—\n",
        "print(f\"ğŸ“‚ æ­£åœ¨æ£€æŸ¥ç›®å½•: {DATASET_FOLDER}\")\n",
        "files = os.listdir(DATASET_FOLDER)\n",
        "print(\"ğŸ“„ å®é™…æ–‡ä»¶åˆ—è¡¨ (å‰3ä¸ª):\", files[:3])\n",
        "\n",
        "# å¦‚æœæ–‡ä»¶æ˜¯ä»¥ 'segment' å¼€å¤´çš„ï¼Œé‚£ä¹ˆä¹‹å‰çš„ validation.tfrecord* å½“ç„¶åŒ¹é…ä¸åˆ°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9hkOKUd6w8QG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“‚ æ•°æ®ç›®å½•: /mnt/d/Datasets/WOD_E2E_Camera_v1/val\n",
            "ğŸ“Š æ‰¾åˆ°æ–‡ä»¶æ•°é‡: 93\n",
            "ğŸ“„ ç¬¬ä¸€ä¸ªæ–‡ä»¶: val_202504211843.tfrecord-00000-of-00093\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-01-05 15:20:51.552451: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2026-01-05 15:20:51.552749: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "Skipping registering GPU devices...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ‰ æˆåŠŸè¯»å–åˆ°æ•°æ®ï¼å¯ä»¥ç»§ç»­è¿è¡Œåç»­çš„ Parse ä»£ç äº†ã€‚\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "import tensorflow as tf\n",
        "\n",
        "# ä½ çš„æ•°æ®ç›®å½•\n",
        "DATASET_FOLDER = '/mnt/d/Datasets/WOD_E2E_Camera_v1/val'\n",
        "\n",
        "print(f\"ğŸ“‚ æ•°æ®ç›®å½•: {DATASET_FOLDER}\")\n",
        "\n",
        "# âŒ åŸæ¥çš„å†™æ³• (åŒ¹é… validation å¼€å¤´)\n",
        "# VALIDATION_FILES = os.path.join(DATASET_FOLDER, 'validation.tfrecord*')\n",
        "\n",
        "# âœ… ä¿®æ­£åçš„å†™æ³• (åŒ¹é… val_ å¼€å¤´ï¼Œä¸”åŒ…å« .tfrecord çš„æ–‡ä»¶)\n",
        "# æ³¨æ„ï¼šä½ çš„æ–‡ä»¶ååé¢è¿˜æœ‰ -00000-of-00093 è¿™ç§åç¼€ï¼Œæ‰€ä»¥æœ«å°¾å¿…é¡»åŠ  *\n",
        "VALIDATION_FILES = os.path.join(DATASET_FOLDER, 'val_*.tfrecord*')\n",
        "\n",
        "# ä½¿ç”¨ glob è·å–çœŸå®æ–‡ä»¶åˆ—è¡¨ï¼ˆç”¨äºè°ƒè¯•ï¼‰\n",
        "filenames = glob.glob(VALIDATION_FILES)\n",
        "print(f\"ğŸ“Š æ‰¾åˆ°æ–‡ä»¶æ•°é‡: {len(filenames)}\")\n",
        "\n",
        "if len(filenames) > 0:\n",
        "    print(f\"ğŸ“„ ç¬¬ä¸€ä¸ªæ–‡ä»¶: {os.path.basename(filenames[0])}\")\n",
        "    \n",
        "    # åˆ›å»ºæ•°æ®é›†\n",
        "    dataset = tf.data.TFRecordDataset(filenames, compression_type='')\n",
        "    dataset_iter = iter(dataset)\n",
        "    \n",
        "    # å†æ¬¡å°è¯•è¯»å–\n",
        "    try:\n",
        "        bytes_example = next(dataset_iter)\n",
        "        print(\"ğŸ‰ æˆåŠŸè¯»å–åˆ°æ•°æ®ï¼å¯ä»¥ç»§ç»­è¿è¡Œåç»­çš„ Parse ä»£ç äº†ã€‚\")\n",
        "    except StopIteration:\n",
        "        print(\"âŒ æ–‡ä»¶æ‰¾åˆ°äº†ï¼Œä½†è¯»å–ä¾ç„¶ä¸ºç©ºã€‚å¯èƒ½éœ€è¦æ£€æŸ¥ compression_typeã€‚\")\n",
        "else:\n",
        "    print(\"âŒ ä¾ç„¶æ²¡æœ‰æ‰¾åˆ°æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶åæ¨¡å¼æ˜¯å¦ä¸ 'val_*.tfrecord*' åŒ¹é…ã€‚\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgTIikH9Bhv6"
      },
      "source": [
        "Initialize dataset object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "mV_m-oc2Bbsn"
      },
      "outputs": [],
      "source": [
        "filenames = tf.io.matching_files(VALIDATION_FILES)\n",
        "dataset = tf.data.TFRecordDataset(filenames, compression_type='')\n",
        "dataset_iter = dataset.as_numpy_iterator()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWj7JeomCy1s"
      },
      "source": [
        "Retrieve one example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3GIvytJ6BqHi"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ‰ è§£ææˆåŠŸï¼\n"
          ]
        }
      ],
      "source": [
        "# è·å–æ•°æ®\n",
        "# æ³¨æ„ï¼šå¦‚æœä½ è¿™é‡ŒæŠ¥é”™ StopIterationï¼Œè¯·é‡æ–°è¿è¡Œ dataset_iter = iter(dataset)\n",
        "bytes_example = next(dataset_iter)\n",
        "\n",
        "# --- æ™ºèƒ½å…¼å®¹å¤„ç† ---\n",
        "if hasattr(bytes_example, 'numpy'):\n",
        "    # å¦‚æœæ˜¯ Tensorï¼Œå°±è½¬æˆ bytes\n",
        "    raw_bytes = bytes_example.numpy()\n",
        "else:\n",
        "    # å¦‚æœå·²ç»æ˜¯ bytesï¼Œå°±ç›´æ¥ç”¨\n",
        "    raw_bytes = bytes_example\n",
        "# --------------------\n",
        "\n",
        "# åˆå§‹åŒ– Protobuf å¯¹è±¡\n",
        "data = wod_e2ed_pb2.E2EDFrame()\n",
        "\n",
        "# è§£æ\n",
        "data.ParseFromString(raw_bytes)\n",
        "\n",
        "print(\"ğŸ‰ è§£ææˆåŠŸï¼\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaXCcjcVP60q"
      },
      "source": [
        "## Visualizing the future trajectories on image\n",
        "In this tutorial, we will visualize a single camera image and project the trajectory on the three front cameras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "rLv0k_XlTULt"
      },
      "outputs": [],
      "source": [
        "def return_front3_cameras(data: wod_e2ed_pb2.E2EDFrame):\n",
        "  \"\"\"Return the front_left, front, and front_right cameras as a list of images\"\"\"\n",
        "  image_list = []\n",
        "  calibration_list = []\n",
        "  # CameraName Enum reference:\n",
        "  # https://github.com/waymo-research/waymo-open-dataset/blob/5f8a1cd42491210e7de629b6f8fc09b65e0cbe99/src/waymo_open_dataset/dataset.proto#L50\n",
        "  order = [2, 1, 3]\n",
        "  for camera_name in order:\n",
        "    for index, image_content in enumerate(data.frame.images):\n",
        "      if image_content.name == camera_name:\n",
        "        # Decode the raw image string and convert to numpy type.\n",
        "        calibration = data.frame.context.camera_calibrations[index]\n",
        "        image = tf.io.decode_image(image_content.image).numpy()\n",
        "        image_list.append(image)\n",
        "        calibration_list.append(calibration)\n",
        "        break\n",
        "\n",
        "  return image_list, calibration_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Yfg6ceNTJwq"
      },
      "source": [
        "Visualize the front 3 cameras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xFePW7Y5WZik"
      },
      "outputs": [],
      "source": [
        "import matplotlib\n",
        "matplotlib.rcParams.update(matplotlib.rcParamsDefault)\n",
        "matplotlib.use('TkAgg', force=True)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# å‡è®¾ data å·²ç»é‡æ–°åŠ è½½å¥½äº†\n",
        "front3_camera_image_list, front3_camera_calibration_list = return_front3_cameras(data)\n",
        "concatenated_image = np.concatenate(front3_camera_image_list, axis=1)\n",
        "\n",
        "plt.figure(figsize=(20, 20))\n",
        "plt.imshow(concatenated_image)\n",
        "plt.axis('off')  # å»ºè®®åŠ ä¸Šè¿™è¡Œï¼Œå…³æ‰åæ ‡è½´æ›´å¥½çœ‹\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "UV6sHoX1w2-k"
      },
      "outputs": [],
      "source": [
        "def project_vehicle_to_image(vehicle_pose, calibration, points):\n",
        "  \"\"\"Projects from vehicle coordinate system to image with global shutter.\n",
        "\n",
        "  Arguments:\n",
        "    vehicle_pose: Vehicle pose transform from vehicle into world coordinate\n",
        "      system.\n",
        "    calibration: Camera calibration details (including intrinsics/extrinsics).\n",
        "    points: Points to project of shape [N, 3] in vehicle coordinate system.\n",
        "\n",
        "  Returns:\n",
        "    Array of shape [N, 3], with the latter dimension composed of (u, v, ok).\n",
        "  \"\"\"\n",
        "  # Transform points from vehicle to world coordinate system (can be\n",
        "  # vectorized).\n",
        "  pose_matrix = np.array(vehicle_pose.transform).reshape(4, 4)\n",
        "  world_points = np.zeros_like(points)\n",
        "  for i, point in enumerate(points):\n",
        "    cx, cy, cz, _ = np.matmul(pose_matrix, [*point, 1])\n",
        "    world_points[i] = (cx, cy, cz)\n",
        "\n",
        "  # Populate camera image metadata. Velocity and latency stats are filled with\n",
        "  # zeroes.\n",
        "  extrinsic = tf.reshape(\n",
        "      tf.constant(list(calibration.extrinsic.transform), dtype=tf.float32),\n",
        "      [4, 4])\n",
        "  intrinsic = tf.constant(list(calibration.intrinsic), dtype=tf.float32)\n",
        "  metadata = tf.constant([\n",
        "      calibration.width,\n",
        "      calibration.height,\n",
        "      open_dataset.CameraCalibration.GLOBAL_SHUTTER,\n",
        "  ],\n",
        "                         dtype=tf.int32)\n",
        "  camera_image_metadata = list(vehicle_pose.transform) + [0.0] * 10\n",
        "\n",
        "  # Perform projection and return projected image coordinates (u, v, ok).\n",
        "  return py_camera_model_ops.world_to_image(extrinsic, intrinsic, metadata,\n",
        "                                            camera_image_metadata,\n",
        "                                            world_points).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "UAgH_pmoxrJo"
      },
      "outputs": [],
      "source": [
        "def draw_points_on_image(image, points, size):\n",
        "  \"\"\"Draws points on an image.\n",
        "\n",
        "  Args:\n",
        "    image: The image to draw on.\n",
        "    points: A numpy array of shape (N, 2) representing the points to draw.\n",
        "  \"\"\"\n",
        "  for point in points:\n",
        "    cv2.circle(image, (int(point[0]), int(point[1])), size, (255, 0, 0), -1)\n",
        "  return image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nL0uiCL1G4y6"
      },
      "source": [
        "Extract the ego vehicle's future trajectory and reshape to (N, 3) matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Go_GRAlVpQJ6"
      },
      "outputs": [],
      "source": [
        "future_waypoints_matrix = np.stack([data.future_states.pos_x, data.future_states.pos_y, data.future_states.pos_z], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrEuJN4Lm5-7"
      },
      "source": [
        "The pose is always an identity matrix as we already convert world coordinates to vehicle coordinates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "kqpRRhoGjbGb"
      },
      "outputs": [],
      "source": [
        "vehicle_pose = data.frame.images[0].pose"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tucgPv_UoDTp"
      },
      "source": [
        "We convert the ego vehicle's future waypoints to camera space and draw on camera."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "UKzDz0NLezyG"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7e22f6bb3460>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "images_with_drawn_points = []\n",
        "for i in range(len(front3_camera_calibration_list)):\n",
        "  waypoints_camera_space = project_vehicle_to_image(vehicle_pose, front3_camera_calibration_list[i], future_waypoints_matrix)\n",
        "  images_with_drawn_points.append(draw_points_on_image(front3_camera_image_list[i], waypoints_camera_space, size=15))\n",
        "concatenated_image = np.concatenate(images_with_drawn_points, axis=1)\n",
        "plt.figure(figsize=(20, 20))\n",
        "plt.imshow(concatenated_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUh5UBgM4rv-"
      },
      "source": [
        "## Submission generation\n",
        "\n",
        "The `wod_e2ed_submission_pb2` defines the proto format of the submission.\n",
        "\n",
        "The participants are required to produce **a single trajectory** starting after the last provided frame. The trajectory should follow  `TrajectoryPrediction` format and has a length of 5 seconds and a frequency of 4 HZ. Then the participants should add the corresponding frame name to form `FrameTrajectoryPredictions`.  The evaluation server will compute detailed metrics and add them to the leaderboard.\n",
        "\n",
        "This section will demonstrate how submission file is created"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "x1SJS4kc_mot"
      },
      "outputs": [],
      "source": [
        "# Assume we have a predicted stopping trajectory.\n",
        "predicted_trajectory = wod_e2ed_submission_pb2.TrajectoryPrediction(pos_x=np.zeros(20, dtype=np.float32),\n",
        "                                                                    pos_y=np.zeros(20, dtype=np.float32))\n",
        "frame_name = data.frame.context.name\n",
        "frame_trajectory = wod_e2ed_submission_pb2.FrameTrajectoryPredictions(frame_name=frame_name, trajectory=predicted_trajectory)\n",
        "# The final prediction should be a list of FrameTrajectoryPredictions.\n",
        "predictions = [frame_trajectory]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "dtAFtomxBFpg"
      },
      "outputs": [],
      "source": [
        "# Pack for submission.\n",
        "num_submission_shards = 1  # Please modify accordingly.\n",
        "submission_file_base = '/tmp/MySubmission'  # Please modify accordingly.\n",
        "if not os.path.exists(submission_file_base):\n",
        "  os.makedirs(submission_file_base)\n",
        "sub_file_names = [\n",
        "    os.path.join(submission_file_base, part)\n",
        "    for part in [f'part{i}' for i in range(num_submission_shards)]\n",
        "]\n",
        "# As the submission file may be large, we shard them into different chunks.\n",
        "submissions = []\n",
        "num_predictions_per_shard =  math.ceil(len(predictions) / num_submission_shards)\n",
        "for i in range(num_submission_shards):\n",
        "  start = i * num_predictions_per_shard\n",
        "  end = (i + 1) * num_predictions_per_shard\n",
        "  submissions.append(\n",
        "      wod_e2ed_submission_pb2.E2EDChallengeSubmission(\n",
        "          predictions=predictions[start:end]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "dqwJXQhdCcGn"
      },
      "outputs": [],
      "source": [
        "for i, shard in enumerate(submissions):\n",
        "  shard.submission_type  =  wod_e2ed_submission_pb2.E2EDChallengeSubmission.SubmissionType.E2ED_SUBMISSION\n",
        "  shard.authors[:] = ['A', 'B']  # Please modify accordingly.\n",
        "  shard.affiliation = 'Affiliation'  # Please modify accordingly.\n",
        "  shard.account_name = 'acc@domain.com'  # Please modify accordingly.\n",
        "  shard.unique_method_name = 'YourMethodName'  # Please modify accordingly.\n",
        "  shard.method_link = 'method_link'  # Please modify accordingly.\n",
        "  shard.description = ''  # Please modify accordingly.\n",
        "  shard.uses_public_model_pretraining = True # Please modify accordingly.\n",
        "  shard.public_model_names.extend(['Model_name']) # Please modify accordingly.\n",
        "  shard.num_model_parameters = \"200k\" # Please modify accordingly.\n",
        "  with tf.io.gfile.GFile(sub_file_names[i], 'wb') as fp:\n",
        "    fp.write(shard.SerializeToString())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "TqmlWbYFEPLO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "predictions {\n",
            "  frame_name: \"11d68b183960928432c0ab7af24ac86d-058\"\n",
            "  trajectory {\n",
            "    pos_x: 0.0\n",
            "    pos_x: 0.0\n",
            "    pos_x: 0.0\n",
            "    pos_x: 0.0\n",
            "    pos_x: 0.0\n",
            "    pos_x: 0.0\n",
            "    pos_x: 0.0\n",
            "    pos_x: 0.0\n",
            "    pos_x: 0.0\n",
            "    pos_x: 0.0\n",
            "    pos_x: 0.0\n",
            "    pos_x: 0.0\n",
            "    pos_x: 0.0\n",
            "    pos_x: 0.0\n",
            "    pos_x: 0.0\n",
            "    pos_x: 0.0\n",
            "    pos_x: 0.0\n",
            "    pos_x: 0.0\n",
            "    pos_x: 0.0\n",
            "    pos_x: 0.0\n",
            "    pos_y: 0.0\n",
            "    pos_y: 0.0\n",
            "    pos_y: 0.0\n",
            "    pos_y: 0.0\n",
            "    pos_y: 0.0\n",
            "    pos_y: 0.0\n",
            "    pos_y: 0.0\n",
            "    pos_y: 0.0\n",
            "    pos_y: 0.0\n",
            "    pos_y: 0.0\n",
            "    pos_y: 0.0\n",
            "    pos_y: 0.0\n",
            "    pos_y: 0.0\n",
            "    pos_y: 0.0\n",
            "    pos_y: 0.0\n",
            "    pos_y: 0.0\n",
            "    pos_y: 0.0\n",
            "    pos_y: 0.0\n",
            "    pos_y: 0.0\n",
            "    pos_y: 0.0\n",
            "  }\n",
            "}\n",
            "submission_type: E2ED_SUBMISSION\n",
            "account_name: \"acc@domain.com\"\n",
            "unique_method_name: \"YourMethodName\"\n",
            "authors: \"A\"\n",
            "authors: \"B\"\n",
            "affiliation: \"Affiliation\"\n",
            "description: \"\"\n",
            "method_link: \"method_link\"\n",
            "uses_public_model_pretraining: true\n",
            "num_model_parameters: \"200k\"\n",
            "public_model_names: \"Model_name\"\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(submissions[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLVzBJxXw33V"
      },
      "source": [
        "## Package submission\n",
        "```\n",
        "cd /tmp\n",
        "tar cvf MySubmission.tar MySubmission\n",
        "gzip MySubmission.tar\n",
        "```\n",
        "Then you can upload `/tmp/MySubmission.tar.gz` to the challenge website.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaQ6UFQaz4fH"
      },
      "source": [
        "## Evaluation\n",
        "Once the predictions are submitted, our eval server will run the [rater feedback metric](https://waymo.com/intl/en_us/open/challenges/2025/e2e-driving/) to compute the rater feedback scores and update the leaderboard. As the rater feedback metric code won't be released, here we provide a simple ADE Metric implementation to help participants self-evaluate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "gMeLb0pX6s1e"
      },
      "outputs": [],
      "source": [
        "def average_distance_per_step(\n",
        "    predictions , observed_traj, mask\n",
        ") :\n",
        "  \"\"\"Compute L2 distance between proposed trajectories and ground truth.\n",
        "\n",
        "  Args:\n",
        "    predictions: A numpy array representing model predictions of size: [# proposals,\n",
        "      # time steps, spatial features].\n",
        "    observed_traj: A tensor representing the observed trajectory in the logs of\n",
        "      size [# time steps, spatial features]\n",
        "    mask: A boolean tensor representing the time steps that have valid\n",
        "      observations of size [# time steps].\n",
        "\n",
        "  Returns:\n",
        "    A tensor of size [# proposals]\n",
        "  \"\"\"\n",
        "  dist_per_step = np.linalg.norm(\n",
        "      predictions - observed_traj[np.newaxis], axis=-1\n",
        "  )\n",
        "  dist_per_traj = (dist_per_step * mask[np.newaxis]).sum(axis=-1)\n",
        "  valid_steps = np.maximum(mask.sum(axis=-1, keepdims=True), 1.0)\n",
        "  avg_distance = dist_per_traj / valid_steps\n",
        "  return avg_distance\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeNkBSQI_A8h"
      },
      "source": [
        "Convert both gt and predictions to a format of dictionary for convenience."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "UyHfj-pz_AU5"
      },
      "outputs": [],
      "source": [
        "# {frame_name: trajectory}\n",
        "gt_dict = {}\n",
        "prediction_dict = {}\n",
        "gt_dict[data.frame.context.name] =  np.stack([data.future_states.pos_x, data.future_states.pos_y], axis=1)\n",
        "for submission in submissions:\n",
        "  for prediction in submission.predictions:\n",
        "    prediction_dict[prediction.frame_name] = np.stack([prediction.trajectory.pos_x, prediction.trajectory.pos_y], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "rJhRZ2knDKOe"
      },
      "outputs": [],
      "source": [
        "# Compute ADEs. Our ade calculation is generalized to multiple proposal and masking enabled.\n",
        "ade_list = []\n",
        "for frame_name in gt_dict:\n",
        "  if frame_name not in prediction_dict:\n",
        "    raise ValueError(f'No prediction for {frame_name}')\n",
        "  gt_traj = gt_dict[frame_name]\n",
        "  pred_traj = prediction_dict[frame_name]\n",
        "  mask = np.ones(gt_traj.shape[0], dtype=np.bool_)\n",
        "  ade_list.append(average_distance_per_step(pred_traj[None], gt_traj, mask)[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "UYkS4cgDA0B_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ADE: 16.591499070772038\n"
          ]
        }
      ],
      "source": [
        "print(f'ADE: {np.mean(ade_list)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "2VrGnxGulZLw"
      },
      "outputs": [],
      "source": [
        "# @title The following code shows how to calculate the rater feedback metric.\n",
        "# First, we need to iterate over the dataset to find the frame that contains\n",
        "# the rated trajectory. Usually in a whole run, only one frame contains such label.\n",
        "data_contain_label = None\n",
        "for raw_data in dataset_iter:\n",
        "  data = wod_e2ed_pb2.E2EDFrame()\n",
        "  data.ParseFromString(raw_data)\n",
        "  if len(data.preference_trajectories) > 0 and \\\n",
        "    data.preference_trajectories[0].preference_score != -1:\n",
        "    data_contain_label = data\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "cH_MFUZSnqZm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "d5fa64f6a8c44c60aaec7b07776e23c4-150\n"
          ]
        }
      ],
      "source": [
        "print(data_contain_label.frame.context.name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Mf_BT47BDw5K"
      },
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "cannot import name 'rater_feedback_utils' from 'waymo_open_dataset.metrics.python' (unknown location)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[23], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# @title The following code shows how to calculate the rater feedback metric.\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwaymo_open_dataset\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m rater_feedback_utils\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Assume we have a list of data.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m data_list \u001b[38;5;241m=\u001b[39m [data_contain_label]\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'rater_feedback_utils' from 'waymo_open_dataset.metrics.python' (unknown location)"
          ]
        }
      ],
      "source": [
        "# @title The following code shows how to calculate the rater feedback metric.\n",
        "from waymo_open_dataset.metrics.python import rater_feedback_utils\n",
        "# Assume we have a list of data.\n",
        "data_list = [data_contain_label]\n",
        "rater_specified_trajectories = []\n",
        "rater_scores = []\n",
        "initial_speed = []\n",
        "\n",
        "prediction_trajectories = []\n",
        "prediction_probabilities = []\n",
        "\n",
        "for i in range(len(data_list)):\n",
        "  rater_specified_trajs_and_scores_i = data_list[i].preference_trajectories\n",
        "  current_rater_trajs = []\n",
        "  current_rater_scores = []\n",
        "  for j in range(len(rater_specified_trajs_and_scores_i)):\n",
        "    current_rater_trajs.append(\n",
        "        np.stack(\n",
        "            [\n",
        "                rater_specified_trajs_and_scores_i[j].pos_x,\n",
        "                rater_specified_trajs_and_scores_i[j].pos_y,\n",
        "            ],\n",
        "            axis=-1,\n",
        "        )\n",
        "    )\n",
        "    current_rater_scores.append(rater_specified_trajs_and_scores_i[j].preference_score)\n",
        "  current_rater_scores = np.array(current_rater_scores)\n",
        "\n",
        "  # Initial speed calculation. The last position of past velocity should be\n",
        "  # the current velocity.\n",
        "  vel_x = data.past_states.vel_x[-1]\n",
        "  vel_y = data.past_states.vel_y[-1]\n",
        "  initial_speed.append(np.sqrt(vel_x**2 + vel_y**2))\n",
        "\n",
        "  # Fake prediction.\n",
        "  prediction_traj = np.zeros((20, 2))\n",
        "  # We add an empty axis as the # of proposals is 1\n",
        "  prediction_trajectories.append(prediction_traj[None])\n",
        "  prediction_probabilities.append(np.ones(1))\n",
        "  # Append the current trajectory and score to the batch list.\n",
        "  rater_specified_trajectories.append(current_rater_trajs)\n",
        "  rater_scores.append(current_rater_scores)\n",
        "\n",
        "# Convert the list of numpy array to add batch dimension.\n",
        "initial_speed = np.stack(initial_speed)\n",
        "prediction_trajectories = np.stack(prediction_trajectories)\n",
        "prediction_probabilities = np.stack(prediction_probabilities)\n",
        "\n",
        "rater_feedback_metrics = (\n",
        "        rater_feedback_utils.get_rater_feedback_score(\n",
        "            prediction_trajectories,\n",
        "            prediction_probabilities,\n",
        "            rater_specified_trajectories,\n",
        "            rater_scores,\n",
        "            initial_speed,\n",
        "            frequency=4,  # Default is 4.\n",
        "            length_seconds=5, # Default predict 5 seconds.\n",
        "            output_trust_region_visualization=False,\n",
        "        )\n",
        "    )\n",
        "print(rater_feedback_metrics['rater_feedback_score'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3q2oFEpqE1y"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "",
        "kind": "local"
      },
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "waymo_clean",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.25"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
