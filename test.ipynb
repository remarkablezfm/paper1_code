{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4d0afd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-05 17:24:54.891341: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-01-05 17:24:54.893600: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2026-01-05 17:24:54.932654: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2026-01-05 17:24:54.934259: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-01-05 17:24:55.469085: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ç¯å¢ƒåŠ è½½æˆåŠŸï¼\n",
      "   - TensorFlow: 2.13.0\n",
      "   - Camera Ops: å·²åŠ è½½\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "\n",
    "# 1. è§£å†³ç»˜å›¾åç«¯é—®é¢˜ (ä¿ç•™ä½ ä¹‹å‰çš„ TkAgg ä¿®å¤)\n",
    "matplotlib.rcParams.update(matplotlib.rcParamsDefault)\n",
    "matplotlib.use('TkAgg', force=True)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 2. å¯¼å…¥ Waymo æ•°æ®å®šä¹‰\n",
    "try:\n",
    "    from waymo_open_dataset import dataset_pb2 as open_dataset\n",
    "    # å°è¯•å¯¼å…¥ E2E æ•°æ®å®šä¹‰\n",
    "    from waymo_open_dataset.protos import end_to_end_driving_data_pb2 as wod_e2ed_pb2\n",
    "    # âš ï¸ å…³é”®ï¼šå¯¼å…¥å®˜æ–¹ Demo ä½¿ç”¨çš„ç›¸æœºæŠ•å½±ç®—å­\n",
    "    from waymo_open_dataset.wdl_limited.camera.ops import py_camera_model_ops\n",
    "    print(f\"âœ… ç¯å¢ƒåŠ è½½æˆåŠŸï¼\")\n",
    "    print(f\"   - TensorFlow: {tf.__version__}\")\n",
    "    print(f\"   - Camera Ops: å·²åŠ è½½\")\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ å¯¼å…¥å¤±è´¥: {e}\")\n",
    "    print(\"æç¤º: è¯·ç¡®ä¿å®‰è£…äº† waymo-open-dataset-tf-2-12-0 (æˆ–å…¼å®¹ç‰ˆæœ¬)\")\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1a0f034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ è¯»å–æ–‡ä»¶: /mnt/d/Datasets/WOD_E2E_Camera_v1/val/val_202504211843.tfrecord-00008-of-00093\n",
      "âœ… æ•°æ®é›†è¿­ä»£å™¨å·²å°±ç»ª\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-05 17:24:56.691323: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2026-01-05 17:24:56.692898: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "# ä¿®æ”¹ä¸ºä½ çš„å®é™…æ–‡ä»¶è·¯å¾„\n",
    "FILENAME = r'D:\\Datasets\\WOD_E2E_Camera_v1\\val\\val_202504211843.tfrecord-00008-of-00093'\n",
    "\n",
    "# è·¯å¾„å…¼å®¹å¤„ç† (WSL vs Windows)\n",
    "if not os.path.exists(FILENAME) and 'mnt' not in FILENAME:\n",
    "    # å°è¯•è‡ªåŠ¨è½¬æ¢ä¸º WSL è·¯å¾„\n",
    "    FILENAME = FILENAME.replace('D:', '/mnt/d').replace('\\\\', '/')\n",
    "\n",
    "print(f\"ğŸ“‚ è¯»å–æ–‡ä»¶: {FILENAME}\")\n",
    "\n",
    "try:\n",
    "    dataset = tf.data.TFRecordDataset(FILENAME, compression_type='')\n",
    "    dataset_iter = iter(dataset)\n",
    "    print(\"âœ… æ•°æ®é›†è¿­ä»£å™¨å·²å°±ç»ª\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ æ–‡ä»¶è¯»å–é”™è¯¯: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b2b8640",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_front3_cameras(data: wod_e2ed_pb2.E2EDFrame):\n",
    "    \"\"\"\n",
    "    æå–å‰å‘ä¸‰ä¸ªæ‘„åƒå¤´çš„æ•°æ® (Image) å’Œ æ ‡å®šå‚æ•° (Calibration)\n",
    "    \"\"\"\n",
    "    image_list = []\n",
    "    calibration_list = []\n",
    "    \n",
    "    # é¡ºåº: å·¦(2) -> ä¸­(1) -> å³(3)\n",
    "    order = [2, 1, 3] \n",
    "    \n",
    "    # å»ºç«‹æŸ¥æ‰¾è¡¨ä»¥åŠ å¿«é€Ÿåº¦\n",
    "    img_dict = {img.name: img for img in data.frame.images}\n",
    "    calib_dict = {cal.name: cal for cal in data.frame.context.camera_calibrations}\n",
    "\n",
    "    for camera_name in order:\n",
    "        if camera_name in img_dict:\n",
    "            # 1. è§£ç å›¾ç‰‡\n",
    "            image_content = img_dict[camera_name]\n",
    "            image = tf.io.decode_image(image_content.image).numpy()\n",
    "            image_list.append(image)\n",
    "            \n",
    "            # 2. è·å–å¯¹åº”çš„æ ‡å®šå‚æ•° (è¿™å¯¹æŠ•å½±è‡³å…³é‡è¦)\n",
    "            if camera_name in calib_dict:\n",
    "                calibration_list.append(calib_dict[camera_name])\n",
    "            else:\n",
    "                print(f\"âš ï¸ è­¦å‘Š: ç¼ºå°‘ç›¸æœº {camera_name} çš„æ ‡å®šæ•°æ®\")\n",
    "                \n",
    "    return image_list, calibration_list\n",
    "\n",
    "def project_vehicle_to_image(vehicle_pose, calibration, points):\n",
    "    \"\"\"\n",
    "    å°†è½¦è¾†åæ ‡ç³»ä¸‹çš„ 3D ç‚¹æŠ•å½±åˆ° 2D å›¾åƒå¹³é¢\n",
    "    æ¥æº: å®˜æ–¹ tutorial_vision_based_e2e_driving.ipynb\n",
    "    \"\"\"\n",
    "    # 1. æ„å»ºå˜æ¢çŸ©é˜µ (Vehicle -> World)\n",
    "    pose_matrix = np.array(vehicle_pose.transform).reshape(4, 4)\n",
    "    \n",
    "    # 2. å°†ç‚¹ä»è½¦è¾†åæ ‡ç³»è½¬åˆ°ä¸–ç•Œåæ ‡ç³»\n",
    "    # points shape: [N, 3]\n",
    "    world_points = np.zeros_like(points)\n",
    "    for i, point in enumerate(points):\n",
    "        # é½æ¬¡åæ ‡å˜æ¢\n",
    "        cx, cy, cz, _ = np.matmul(pose_matrix, [*point, 1])\n",
    "        world_points[i] = (cx, cy, cz)\n",
    "\n",
    "    # 3. å‡†å¤‡ç›¸æœºå‚æ•° Tensor\n",
    "    extrinsic = tf.reshape(\n",
    "        tf.constant(list(calibration.extrinsic.transform), dtype=tf.float32),\n",
    "        [4, 4])\n",
    "    intrinsic = tf.constant(list(calibration.intrinsic), dtype=tf.float32)\n",
    "    \n",
    "    metadata = tf.constant([\n",
    "        calibration.width,\n",
    "        calibration.height,\n",
    "        open_dataset.CameraCalibration.GLOBAL_SHUTTER,\n",
    "    ], dtype=tf.int32)\n",
    "    \n",
    "    # ç›¸æœºè‡ªèº«çš„ä½å§¿ + å ä½ç¬¦\n",
    "    camera_image_metadata = list(vehicle_pose.transform) + [0.0] * 10\n",
    "\n",
    "    # 4. è°ƒç”¨ Waymo ä¸“ç”¨ç®—å­è¿›è¡ŒæŠ•å½±\n",
    "    # è¿”å›: [N, 3] -> (u, v, ok) å…¶ä¸­ ok=1 è¡¨ç¤ºåœ¨å›¾åƒå†…\n",
    "    return py_camera_model_ops.world_to_image(\n",
    "        extrinsic, intrinsic, metadata,\n",
    "        camera_image_metadata,\n",
    "        world_points\n",
    "    ).numpy()\n",
    "\n",
    "def draw_points_on_image(image, points, color=(255, 0, 0), size=5):\n",
    "    \"\"\"åœ¨å›¾åƒä¸Šç”»ç‚¹\"\"\"\n",
    "    # å¤åˆ¶ä¸€ä»½ä»¥é˜²ä¿®æ”¹åŸå›¾\n",
    "    canvas = image.copy()\n",
    "    for point in points:\n",
    "        u, v, ok = point\n",
    "        if ok: # åªç”»åœ¨å›¾åƒèŒƒå›´å†…çš„ç‚¹\n",
    "            cv2.circle(canvas, (int(u), int(v)), size, color, -1)\n",
    "    return canvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3834a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ å½“å‰å¸§ ID: 5bef8f2e82697e56d4151d1462088b90-206\n",
      "ğŸš€ æå–åˆ°æœªæ¥è½¨è¿¹ç‚¹æ•°: 20\n",
      "ğŸ‰ å¯è§†åŒ–å®Œæˆï¼è¯·æŸ¥çœ‹å¼¹å‡ºçš„çª—å£ï¼Œçº¢ç‚¹å³ä¸ºè½¦è¾†çš„æœªæ¥çœŸå€¼è½¨è¿¹ã€‚\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # 1. å–å‡ºä¸€å¸§å¹¶è§£æ\n",
    "    data_bytes = next(dataset_iter)\n",
    "    data = wod_e2ed_pb2.E2EDFrame()\n",
    "    data.ParseFromString(data_bytes.numpy())\n",
    "    \n",
    "    print(f\"ğŸ“ å½“å‰å¸§ ID: {data.frame.context.name}\")\n",
    "\n",
    "    # 2. æå–å›¾ç‰‡å’Œæ ‡å®š\n",
    "    images, calibs = return_front3_cameras(data)\n",
    "    \n",
    "    if len(images) == 3:\n",
    "        # 3. æå–æœªæ¥è½¨è¿¹ (Ground Truth Trajectory)\n",
    "        # å½¢çŠ¶: (N, 3) -> x, y, z\n",
    "        future_waypoints = np.stack([\n",
    "            data.future_states.pos_x, \n",
    "            data.future_states.pos_y, \n",
    "            data.future_states.pos_z\n",
    "        ], axis=1)\n",
    "        \n",
    "        print(f\"ğŸš€ æå–åˆ°æœªæ¥è½¨è¿¹ç‚¹æ•°: {len(future_waypoints)}\")\n",
    "\n",
    "        # 4. è·å–è½¦è¾†å½“å‰ä½å§¿ (é€šå¸¸ç¬¬ä¸€å¼ å›¾çš„ä½å§¿è¿‘ä¼¼ä»£è¡¨è½¦è¾†ä½å§¿)\n",
    "        vehicle_pose = data.frame.images[0].pose\n",
    "\n",
    "        # 5. æŠ•å½±å¹¶ç»˜å›¾\n",
    "        drawn_images = []\n",
    "        for i in range(3):\n",
    "            # A. 3D -> 2D æŠ•å½±\n",
    "            projected_points = project_vehicle_to_image(\n",
    "                vehicle_pose, \n",
    "                calibs[i], \n",
    "                future_waypoints\n",
    "            )\n",
    "            # B. åœ¨å›¾ä¸Šç”»çº¢ç‚¹\n",
    "            img_with_traj = draw_points_on_image(images[i], projected_points, size=15)\n",
    "            drawn_images.append(img_with_traj)\n",
    "\n",
    "        # 6. æ‹¼æ¥å…¨æ™¯\n",
    "        panorama = np.concatenate(drawn_images, axis=1)\n",
    "\n",
    "        # 7. å¼¹çª—æ˜¾ç¤º\n",
    "        plt.figure(figsize=(20, 8))\n",
    "        plt.imshow(panorama)\n",
    "        plt.title(f\"E2E Panorama with Future Trajectory (Red Dots)\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        print(\"ğŸ‰ å¯è§†åŒ–å®Œæˆï¼è¯·æŸ¥çœ‹å¼¹å‡ºçš„çª—å£ï¼Œçº¢ç‚¹å³ä¸ºè½¦è¾†çš„æœªæ¥çœŸå€¼è½¨è¿¹ã€‚\")\n",
    "        \n",
    "    else:\n",
    "        print(\"âš ï¸ æœ¬å¸§æ•°æ®ä¸å®Œæ•´ï¼Œæ— æ³•æ‹¼æ¥ã€‚\")\n",
    "\n",
    "except StopIteration:\n",
    "    print(\"âš ï¸ æ•°æ®å·²è¯»å®Œï¼Œè¯·é‡æ–°è¿è¡Œ Cell 2ã€‚\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ è¿è¡Œå‡ºé”™: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bb0bd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db889860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ è¯»å–æ–‡ä»¶: /mnt/d/Datasets/WOD_E2E_Camera_v1/val/val_sequence_name_to_scenario_cluster.json\n",
      "âœ… æˆåŠŸåŠ è½½ JSONï¼Œå…± 479 æ¡è®°å½•ã€‚\n",
      "\n",
      "ğŸ” === æ•°æ®ç»“æ„ä¾¦æ¢ ===\n",
      "æ¯ä¸€æ¡æ•°æ®çš„å€¼æ˜¯ä¸€ä¸ªå­—å…¸ï¼Œå†…å®¹å¦‚ä¸‹ï¼š\n",
      "{\n",
      "    \"scenario_cluster\": \"Pedestrian\"\n",
      "}\n",
      "======================\n",
      "\n",
      "ğŸ’¡ è‡ªåŠ¨é”å®šç›®æ ‡å­—æ®µ: 'scenario_cluster'\n",
      "\n",
      "ğŸ“Š === ç»Ÿè®¡ç»“æœ (Top 10) ===\n",
      "                       Count  Percentage\n",
      "Interections             116       24.22\n",
      "Foreign Object Debris     78       16.28\n",
      "Cyclist                   71       14.82\n",
      "Pedestrian                52       10.86\n",
      "Multi-Lane Maneuvers      42        8.77\n",
      "Single-Lane Maneuvers     38        7.93\n",
      "Special Vehicles          25        5.22\n",
      "Others                    22        4.59\n",
      "Cut_ins                   20        4.18\n",
      "Construction              15        3.13\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# 1. è·¯å¾„ (WSL æ ¼å¼)\n",
    "FILE_PATH = '/mnt/d/Datasets/WOD_E2E_Camera_v1/val/val_sequence_name_to_scenario_cluster.json'\n",
    "\n",
    "# è§£å†³ç»˜å›¾\n",
    "matplotlib.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans', 'Arial']\n",
    "matplotlib.rcParams['axes.unicode_minus'] = False\n",
    "matplotlib.use('TkAgg', force=True)\n",
    "\n",
    "print(f\"ğŸ“‚ è¯»å–æ–‡ä»¶: {FILE_PATH}\")\n",
    "\n",
    "with open(FILE_PATH, 'r', encoding='utf-8') as f:\n",
    "    data_mapping = json.load(f)\n",
    "\n",
    "print(f\"âœ… æˆåŠŸåŠ è½½ JSONï¼Œå…± {len(data_mapping)} æ¡è®°å½•ã€‚\")\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# ğŸ”¬ æ˜¾å¾®é•œæ¨¡å¼ï¼šçœ‹çœ‹é‡Œé¢åˆ°åº•é•¿ä»€ä¹ˆæ ·\n",
    "# ------------------------------------------------------\n",
    "# è·å–ç¬¬ä¸€ä¸ª value\n",
    "first_value = list(data_mapping.values())[0]\n",
    "print(\"\\nğŸ” === æ•°æ®ç»“æ„ä¾¦æ¢ ===\")\n",
    "print(\"æ¯ä¸€æ¡æ•°æ®çš„å€¼æ˜¯ä¸€ä¸ªå­—å…¸ï¼Œå†…å®¹å¦‚ä¸‹ï¼š\")\n",
    "print(json.dumps(first_value, indent=4))\n",
    "print(\"======================\\n\")\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# ğŸ§  æ™ºèƒ½æå–ä¸ç»Ÿè®¡\n",
    "# ------------------------------------------------------\n",
    "# æˆ‘ä»¬éœ€è¦ä»ä¸Šé¢çš„å­—å…¸é‡Œæ‰¾åˆ°ä»£è¡¨â€œç±»åˆ«â€çš„ Key\n",
    "# å¸¸è§çš„åå­—å¯èƒ½æ˜¯: 'cluster', 'type', 'class', 'category'\n",
    "\n",
    "# å°è¯•æå–å€¼çš„åˆ—è¡¨\n",
    "extracted_values = []\n",
    "\n",
    "# è‡ªåŠ¨çŒœæµ‹ Key (ä½ å¯ä»¥æ ¹æ®ä¸Šé¢æ‰“å°çš„ç»“æœæ‰‹åŠ¨ä¿®æ”¹è¿™é‡Œ)\n",
    "target_key = None\n",
    "if isinstance(first_value, dict):\n",
    "    # ç®€å•çš„çŒœæµ‹é€»è¾‘\n",
    "    for candidate in ['cluster', 'scenario_cluster', 'type', 'category', 'class_name']:\n",
    "        if candidate in first_value:\n",
    "            target_key = candidate\n",
    "            print(f\"ğŸ’¡ è‡ªåŠ¨é”å®šç›®æ ‡å­—æ®µ: '{target_key}'\")\n",
    "            break\n",
    "            \n",
    "    if target_key:\n",
    "        # å¦‚æœæ‰¾åˆ°äº†ï¼Œå°±æå–è¿™ä¸ªå­—æ®µ\n",
    "        extracted_values = [v.get(target_key, 'Unknown') for v in data_mapping.values()]\n",
    "    else:\n",
    "        print(\"âš ï¸ æ²¡çŒœåˆ°å­—æ®µåï¼Œå°†ç»Ÿè®¡æ•´ä¸ªå­—å…¸å†…å®¹çš„å­—ç¬¦ä¸²å½¢å¼...\")\n",
    "        # å…œåº•æ–¹æ¡ˆï¼šæŠŠå­—å…¸è½¬æˆå­—ç¬¦ä¸²ç»Ÿè®¡ï¼Œé˜²æ­¢æŠ¥é”™\n",
    "        extracted_values = [str(v) for v in data_mapping.values()]\n",
    "else:\n",
    "    # å¦‚æœä¸æ˜¯å­—å…¸ï¼ˆåº”è¯¥ä¸ä¼šèµ°åˆ°è¿™ï¼Œå› ä¸ºæŠ¥é”™è¯´æ˜¯dictï¼‰\n",
    "    extracted_values = list(data_mapping.values())\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# ğŸ“Š ç»Ÿè®¡ç»˜å›¾ (å¤ç”¨ä¹‹å‰çš„é€»è¾‘)\n",
    "# ------------------------------------------------------\n",
    "cluster_counts = Counter(extracted_values)\n",
    "df = pd.DataFrame.from_dict(cluster_counts, orient='index', columns=['Count'])\n",
    "df = df.sort_values(by='Count', ascending=False)\n",
    "total = sum(cluster_counts.values())\n",
    "df['Percentage'] = (df['Count'] / total * 100).round(2)\n",
    "\n",
    "print(\"\\nğŸ“Š === ç»Ÿè®¡ç»“æœ (Top 10) ===\")\n",
    "print(df.head(10))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plot_data = df.head(15)\n",
    "plt.bar(plot_data.index.astype(str), plot_data['Count'], color='coral', alpha=0.7)\n",
    "plt.title(f'Scenario Distribution (Key: {target_key if target_key else \"Raw Content\"})')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc02877",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92562464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    "# import tensorflow as tf\n",
    "\n",
    "# # 1. å‡†å¤‡å·¥ä½œ\n",
    "# OUTPUT_VIDEO_NAME = \"waymo_demo_video.mp4\"\n",
    "# FPS = 10 # Waymo æ•°æ®é€šå¸¸æ˜¯ 10Hz\n",
    "# video_writer = None\n",
    "\n",
    "# print(f\"ğŸ¬ å¼€å§‹ç”Ÿæˆè§†é¢‘ï¼Œç›®æ ‡æ–‡ä»¶: {OUTPUT_VIDEO_NAME}\")\n",
    "# print(\"â³è¿™å¯èƒ½éœ€è¦ä¸€ä¸¤åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…...\")\n",
    "\n",
    "# # é‡æ–°åŠ è½½æ•°æ®é›†ï¼ˆç¡®ä¿ä»å¤´å¼€å§‹ï¼‰\n",
    "# try:\n",
    "#     dataset = tf.data.TFRecordDataset(FILENAME, compression_type='')\n",
    "    \n",
    "#     # --- æ ¸å¿ƒå¾ªç¯å¼€å§‹ ---\n",
    "#     # è¿™æ¬¡æˆ‘ä»¬ä¸ç”¨ next()ï¼Œè€Œæ˜¯ç”¨ for å¾ªç¯éå†æ‰€æœ‰æ•°æ®\n",
    "#     for i, data_bytes in enumerate(dataset):\n",
    "        \n",
    "#         # A. è§£ææ•°æ®\n",
    "#         frame = wod_e2ed_pb2.E2EDFrame()\n",
    "#         frame.ParseFromString(data_bytes.numpy())\n",
    "        \n",
    "#         # B. æå–å›¾ç‰‡å’Œæ ‡å®š\n",
    "#         # (å¤ç”¨ä¹‹å‰å®šä¹‰çš„ return_front3_cameras å‡½æ•°)\n",
    "#         images, calibs = return_front3_cameras(frame)\n",
    "        \n",
    "#         if len(images) == 3:\n",
    "#             # C. æå–å¹¶æŠ•å½±è½¨è¿¹ (å¤ç”¨ä¹‹å‰é€»è¾‘)\n",
    "#             future_waypoints = np.stack([\n",
    "#                 frame.future_states.pos_x, \n",
    "#                 frame.future_states.pos_y, \n",
    "#                 frame.future_states.pos_z\n",
    "#             ], axis=1)\n",
    "            \n",
    "#             vehicle_pose = frame.frame.images[0].pose\n",
    "            \n",
    "#             drawn_images = []\n",
    "#             for cam_idx in range(3):\n",
    "#                 # æŠ•å½±\n",
    "#                 proj_points = project_vehicle_to_image(\n",
    "#                     vehicle_pose, calibs[cam_idx], future_waypoints\n",
    "#                 )\n",
    "#                 # ç”»ç‚¹\n",
    "#                 img_drawn = draw_points_on_image(images[cam_idx], proj_points, size=15)\n",
    "#                 drawn_images.append(img_drawn)\n",
    "            \n",
    "#             # D. æ‹¼æ¥å…¨æ™¯\n",
    "#             panorama = np.concatenate(drawn_images, axis=1)\n",
    "            \n",
    "#             # E. è§†é¢‘å†™å…¥å™¨çš„åˆå§‹åŒ– (åªåœ¨ç¬¬ä¸€å¸§åšä¸€æ¬¡)\n",
    "#             if video_writer is None:\n",
    "#                 h, w, c = panorama.shape\n",
    "#                 # OpenCV è§†é¢‘ç¼–ç å™¨è®¾ç½®\n",
    "#                 fourcc = cv2.VideoWriter_fourcc(*'mp4v') \n",
    "#                 video_writer = cv2.VideoWriter(OUTPUT_VIDEO_NAME, fourcc, FPS, (w, h))\n",
    "#                 print(f\"   ğŸ¥ è§†é¢‘å°ºå¯¸å·²ç¡®è®¤ä¸º: {w}x{h}\")\n",
    "\n",
    "#             # F. é¢œè‰²è½¬æ¢å¹¶å†™å…¥\n",
    "#             # Matplotlib æ˜¯ RGBï¼ŒOpenCV æ˜¯ BGRã€‚å¿…é¡»è½¬æ¢ï¼Œå¦åˆ™è§†é¢‘é¢œè‰²å‘è“ã€‚\n",
    "#             panorama_bgr = cv2.cvtColor(panorama, cv2.COLOR_RGB2BGR)\n",
    "#             video_writer.write(panorama_bgr)\n",
    "            \n",
    "#             # æ¯å¤„ç† 20 å¸§æ‰“å°ä¸€æ¬¡è¿›åº¦\n",
    "#             if i % 20 == 0:\n",
    "#                 print(f\"   ...å·²å¤„ç† {i} å¸§\")\n",
    "\n",
    "#     # --- å¾ªç¯ç»“æŸ ---\n",
    "    \n",
    "#     # é‡Šæ”¾èµ„æº\n",
    "#     if video_writer:\n",
    "#         video_writer.release()\n",
    "#         print(f\"\\nâœ… è§†é¢‘ç”Ÿæˆå®Œæ¯•ï¼ä¿å­˜ä¸º: {OUTPUT_VIDEO_NAME}\")\n",
    "#         print(f\"   æ€»å…±å¤„ç†å¸§æ•°: {i+1}\")\n",
    "#     else:\n",
    "#         print(\"âŒ æœªèƒ½ç”Ÿæˆè§†é¢‘ï¼Œå¯èƒ½æ˜¯å› ä¸ºç¬¬ä¸€å¸§å°±æ²¡è¯»åˆ°æ•°æ®ã€‚\")\n",
    "\n",
    "# except Exception as e:\n",
    "#     print(f\"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‡ºé”™: {e}\")\n",
    "#     if video_writer:\n",
    "#         video_writer.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60157580",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00667771",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "waymo_clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
