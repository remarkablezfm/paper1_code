{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f02b8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from waymo_open_dataset import dataset_pb2 as open_dataset\n",
    "from waymo_open_dataset.protos import end_to_end_driving_data_pb2 as wod_e2ed_pb2\n",
    "\n",
    "\n",
    "def parse_e2ed(raw_bytes: bytes):\n",
    "    m = wod_e2ed_pb2.E2EDFrame()\n",
    "    m.ParseFromString(raw_bytes)\n",
    "    return m\n",
    "\n",
    "def split_context_name(ctx_name: str):\n",
    "    \"\"\"'<segment>-<idx>' -> (segment_id, idx:int or None)\"\"\"\n",
    "    if not ctx_name or \"-\" not in ctx_name:\n",
    "        return ctx_name, None\n",
    "    base, suf = ctx_name.rsplit(\"-\", 1)\n",
    "    try:\n",
    "        return base, int(suf)\n",
    "    except ValueError:\n",
    "        return ctx_name, None\n",
    "\n",
    "def get_segment_and_keyidx(e2e):\n",
    "    ctx = e2e.frame.context.name if (hasattr(e2e, \"frame\") and hasattr(e2e.frame, \"context\")) else \"\"\n",
    "    seg_id, key_idx = split_context_name(ctx)\n",
    "    return seg_id, key_idx, ctx\n",
    "\n",
    "def extract_ego_states(states_obj, keys=(\n",
    "    \"pos_x\",\"pos_y\",\"pos_z\",\n",
    "    \"vel_x\",\"vel_y\",\"vel_z\",\n",
    "    \"accel_x\",\"accel_y\",\"accel_z\",\n",
    "    \"heading\"\n",
    ")):\n",
    "    \"\"\"\n",
    "    适配你当前看到的 EgoTrajectoryStates 结构：很多字段是 repeated scalar\n",
    "    返回: dict[k]=np.array(float32), 以及长度 L\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    if states_obj is None:\n",
    "        return out, 0\n",
    "\n",
    "    for k in keys:\n",
    "        if hasattr(states_obj, k):\n",
    "            out[k] = np.array(list(getattr(states_obj, k)), dtype=np.float32)\n",
    "\n",
    "    # 取一个存在字段的长度\n",
    "    L = 0\n",
    "    for v in out.values():\n",
    "        L = len(v)\n",
    "        break\n",
    "    return out, L\n",
    "\n",
    "def extract_ego_from_record(e2e):\n",
    "    \"\"\"\n",
    "    输出一条 record 的 ego 信息（keyframe级别）\n",
    "    \"\"\"\n",
    "    seg_id, key_idx, key_id = get_segment_and_keyidx(e2e)\n",
    "    intent = int(getattr(e2e, \"intent\", -1))\n",
    "\n",
    "    past_dict, past_L = extract_ego_states(getattr(e2e, \"past_states\", None))\n",
    "    fut_dict,  fut_L  = extract_ego_states(getattr(e2e, \"future_states\", None))\n",
    "\n",
    "    return {\n",
    "        \"segment_id\": seg_id,\n",
    "        \"key_idx\": key_idx,\n",
    "        \"key_id\": key_id,\n",
    "        \"intent\": intent,\n",
    "        \"past\": past_dict,\n",
    "        \"past_L\": past_L,\n",
    "        \"future\": fut_dict,\n",
    "        \"future_L\": fut_L,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca762840",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_tfrecords(folder):\n",
    "    files = sorted(glob.glob(os.path.join(folder, \"*.tfrecord*\")))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No tfrecord files under: {folder}\")\n",
    "    return files\n",
    "\n",
    "def collect_segment_ego(val_or_train_folder, target_segment_id, max_files=None):\n",
    "    \"\"\"\n",
    "    返回: records(list)，每个元素是 extract_ego_from_record 的输出\n",
    "    \"\"\"\n",
    "    files = list_tfrecords(val_or_train_folder)\n",
    "    if max_files is not None:\n",
    "        files = files[:max_files]\n",
    "\n",
    "    records = []\n",
    "    for fp in files:\n",
    "        ds = tf.data.TFRecordDataset(fp, compression_type=(\"GZIP\" if fp.endswith(\".gz\") else \"\"))\n",
    "        for r in ds:\n",
    "            e2e = parse_e2ed(r.numpy())\n",
    "            seg_id, key_idx, _ = get_segment_and_keyidx(e2e)\n",
    "            if seg_id != target_segment_id:\n",
    "                continue\n",
    "            rec = extract_ego_from_record(e2e)\n",
    "            if rec[\"key_idx\"] is not None:\n",
    "                records.append(rec)\n",
    "\n",
    "    records.sort(key=lambda x: x[\"key_idx\"])\n",
    "    return records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2b6124c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuse_segment_ego_timeline(records, fps=10, past_includes_keyframe=False):\n",
    "    \"\"\"\n",
    "    把多个 keyframe 的 past/future 投到统一 abs_idx，并融合（同一 idx 多个值取平均）\n",
    "    返回: timeline dict，包含 idx + 各字段数组（缺失为 NaN）\n",
    "    \"\"\"\n",
    "    if not records:\n",
    "        return None\n",
    "\n",
    "    # abs_idx -> field -> list[values]\n",
    "    agg = {}\n",
    "\n",
    "    def _push(abs_i, field, val):\n",
    "        agg.setdefault(abs_i, {}).setdefault(field, []).append(float(val))\n",
    "\n",
    "    for rec in records:\n",
    "        key = rec[\"key_idx\"]\n",
    "        past = rec[\"past\"]\n",
    "        fut  = rec[\"future\"]\n",
    "\n",
    "        # past offsets\n",
    "        if past:\n",
    "            L = rec[\"past_L\"]\n",
    "            if past_includes_keyframe:\n",
    "                offsets = np.arange(-L+1, 1, dtype=int)   # [-L+1 .. 0]\n",
    "            else:\n",
    "                offsets = np.arange(-L, 0, dtype=int)     # [-L .. -1]\n",
    "\n",
    "            for j, off in enumerate(offsets):\n",
    "                abs_i = key + int(off)\n",
    "                for field, arr in past.items():\n",
    "                    if j < len(arr):\n",
    "                        _push(abs_i, field, arr[j])\n",
    "\n",
    "        # future offsets: 默认不包含 keyframe（[1..L]）\n",
    "        if fut:\n",
    "            L = rec[\"future_L\"]\n",
    "            offsets = np.arange(1, L+1, dtype=int)\n",
    "            for j, off in enumerate(offsets):\n",
    "                abs_i = key + int(off)\n",
    "                for field, arr in fut.items():\n",
    "                    if j < len(arr):\n",
    "                        _push(abs_i, field, arr[j])\n",
    "\n",
    "    idxs = sorted(agg.keys())\n",
    "    # 收集全字段集合\n",
    "    fields = sorted({f for i in idxs for f in agg[i].keys()})\n",
    "\n",
    "    out = {\"segment_id\": records[0][\"segment_id\"], \"fps\": fps, \"idx\": np.array(idxs, dtype=int)}\n",
    "    for f in fields:\n",
    "        out[f] = np.array(\n",
    "            [np.mean(agg[i].get(f, [np.nan])) if f in agg[i] else np.nan for i in idxs],\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d449aa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL_DIR = r\"/mnt/d/Datasets/WOD_E2E_Camera_v1/val\"  # 你自己的路径\n",
    "SEG_ID  = \"fb0ed944efebd34d756103188d59da85\"\n",
    "\n",
    "# 1) 收集 keyframe 级 ego status\n",
    "records = collect_segment_ego(VAL_DIR, SEG_ID)\n",
    "print(\"keyframes:\", len(records), \"key_idx range:\", (records[0][\"key_idx\"], records[-1][\"key_idx\"]) if records else None)\n",
    "\n",
    "# 2) 融合成 segment 时间轴轨迹\n",
    "timeline = fuse_segment_ego_timeline(records, fps=10, past_includes_keyframe=False)\n",
    "print(\"timeline idx range:\", (int(timeline[\"idx\"].min()), int(timeline[\"idx\"].max())) if timeline else None)\n",
    "\n",
    "# 3) 保存（npz 最方便）\n",
    "if timeline:\n",
    "    np.savez(f\"{SEG_ID}_ego_cam_free.npz\", **timeline)\n",
    "    print(\"saved:\", f\"{SEG_ID}_ego_cam_free.npz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f65d345",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54558862",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-06 17:28:38.314782: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-01-06 17:28:38.315907: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2026-01-06 17:28:38.336914: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2026-01-06 17:28:38.337555: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-01-06 17:28:38.764882: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def parse_e2ed(raw_bytes: bytes):\n",
    "    m = wod_e2ed_pb2.E2EDFrame()\n",
    "    m.ParseFromString(raw_bytes)\n",
    "    return m\n",
    "\n",
    "def split_context_name(ctx_name: str):\n",
    "    if not ctx_name or \"-\" not in ctx_name:\n",
    "        return ctx_name, None\n",
    "    base, suf = ctx_name.rsplit(\"-\", 1)\n",
    "    try:\n",
    "        return base, int(suf)\n",
    "    except ValueError:\n",
    "        return ctx_name, None\n",
    "\n",
    "def find_record_by_seg_and_idx(tfrecord_file, target_seg_id, target_key_idx, max_scan=None):\n",
    "    \"\"\"\n",
    "    在一个 tfrecord shard 里找某个 segment 的某个 key_idx\n",
    "    找到则返回 e2e，否则返回 None\n",
    "    \"\"\"\n",
    "    ds = tf.data.TFRecordDataset(\n",
    "        tfrecord_file,\n",
    "        compression_type=(\"GZIP\" if tfrecord_file.endswith(\".gz\") else \"\")\n",
    "    )\n",
    "    for i, r in enumerate(ds):\n",
    "        if max_scan is not None and i >= max_scan:\n",
    "            break\n",
    "        e2e = parse_e2ed(r.numpy())\n",
    "        ctx = e2e.frame.context.name\n",
    "        seg_id, key_idx = split_context_name(ctx)\n",
    "        if seg_id == target_seg_id and key_idx == target_key_idx:\n",
    "            return e2e\n",
    "    return None\n",
    "\n",
    "def _get_repeated(states_obj, field):\n",
    "    if states_obj is None or (not hasattr(states_obj, field)):\n",
    "        return None\n",
    "    return np.array(list(getattr(states_obj, field)), dtype=np.float32)\n",
    "\n",
    "def show_ego_status_one_record(e2e, head=5):\n",
    "    \"\"\"\n",
    "    轻量展示：只打印你最关心的 ego status（past/future）\n",
    "    \"\"\"\n",
    "    ctx = e2e.frame.context.name\n",
    "    seg_id, key_idx = split_context_name(ctx)\n",
    "    print(\"=== Keyframe ===\")\n",
    "    print(\"context.name:\", ctx)\n",
    "    print(\"segment_id:\", seg_id)\n",
    "    print(\"key_idx:\", key_idx)\n",
    "    print(\"intent:\", int(getattr(e2e, \"intent\", -1)))\n",
    "\n",
    "    past = getattr(e2e, \"past_states\", None)\n",
    "    fut  = getattr(e2e, \"future_states\", None)\n",
    "\n",
    "    # 你目前版本里常见字段（不保证全都有，所以做了容错）\n",
    "    past_fields = [\"pos_x\",\"pos_y\",\"vel_x\",\"vel_y\",\"accel_x\",\"accel_y\"]\n",
    "    fut_fields  = [\"pos_x\",\"pos_y\",\"pos_z\"]  # 你之前看到 future 有 pos_z\n",
    "\n",
    "    print(\"\\n=== past_states ===\")\n",
    "    for f in past_fields:\n",
    "        arr = _get_repeated(past, f)\n",
    "        if arr is None:\n",
    "            continue\n",
    "        print(f\"{f}: len={len(arr)} head={arr[:head]} tail={arr[-head:]}\")\n",
    "\n",
    "    print(\"\\n=== future_states ===\")\n",
    "    for f in fut_fields:\n",
    "        arr = _get_repeated(fut, f)\n",
    "        if arr is None:\n",
    "            continue\n",
    "        print(f\"{f}: len={len(arr)} head={arr[:head]} tail={arr[-head:]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35a132df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found: True\n",
      "=== Keyframe ===\n",
      "context.name: fb0ed944efebd34d756103188d59da85-223\n",
      "segment_id: fb0ed944efebd34d756103188d59da85\n",
      "key_idx: 223\n",
      "intent: 1\n",
      "\n",
      "=== past_states ===\n",
      "pos_x: len=16 head=[-21.756836 -20.174805 -18.549805 -16.89746  -15.229492 -13.55957 ] tail=[-5.7597656 -4.435547  -3.2021484 -2.0546875 -0.9863281  0.       ]\n",
      "pos_y: len=16 head=[-0.02978516 -0.00634766  0.01025391  0.02392578  0.02734375  0.02880859] tail=[ 0.01220703  0.0078125   0.00292969  0.         -0.00195312  0.        ]\n",
      "vel_x: len=16 head=[6.426236  6.566687  6.650089  6.6838446 6.670882  6.5977283] tail=[5.0934463 4.766901  4.4306293 4.1217246 3.7761056 3.7761056]\n",
      "vel_y: len=16 head=[ 0.10726788  0.09395864  0.06050611  0.02924885  0.01881703 -0.00044798] tail=[-0.01857909 -0.01457623 -0.02092961 -0.0144933  -0.00751033 -0.0075103 ]\n",
      "accel_x: len=16 head=[ 0.20577478  0.14045095  0.08340168  0.03375578 -0.01296234 -0.07315397] tail=[-0.41816998 -0.32654524 -0.33627176 -0.30890465 -0.34561896 -0.34561896]\n",
      "accel_y: len=16 head=[-0.02171137 -0.01330924 -0.03345253 -0.03125726 -0.01043182 -0.01926501] tail=[ 0.00766626  0.00400286 -0.00635338  0.0064363   0.00698297  0.00698304]\n",
      "\n",
      "=== future_states ===\n",
      "pos_x: len=20 head=[0.89941406 1.7099609  2.4296875  3.0546875  3.5800781  3.9970703 ] tail=[4.8876953 5.001953  5.176758  5.413086  5.6992188 6.017578 ]\n",
      "pos_y: len=20 head=[0.00585938 0.01953125 0.04394531 0.07080078 0.09814453 0.12158203] tail=[0.17578125 0.18310547 0.1953125  0.21142578 0.22998047 0.25048828]\n",
      "pos_z: len=20 head=[0.01024628 0.01420593 0.01151276 0.01443481 0.02179718 0.02427673] tail=[0.02017975 0.0196228  0.01985168 0.02129364 0.02532196 0.02913666]\n"
     ]
    }
   ],
   "source": [
    "from waymo_open_dataset import dataset_pb2 as open_dataset\n",
    "from waymo_open_dataset.protos import end_to_end_driving_data_pb2 as wod_e2ed_pb2\n",
    "\n",
    "FILENAME = r\"/mnt/d/Datasets/WOD_E2E_Camera_v1/val/val_202504211843.tfrecord-00008-of-00093\"\n",
    "SEG_ID   = \"fb0ed944efebd34d756103188d59da85\"\n",
    "KEY_IDX  = 223   # 例子：你之前出现过 20~223\n",
    "\n",
    "e2e = find_record_by_seg_and_idx(FILENAME, SEG_ID, KEY_IDX, max_scan=None)\n",
    "print(\"found:\", e2e is not None)\n",
    "\n",
    "if e2e is not None:\n",
    "    show_ego_status_one_record(e2e, head=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930a9a88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "waymo_clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
