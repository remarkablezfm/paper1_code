{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f65d345",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54558862",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def parse_e2ed(raw_bytes: bytes):\n",
    "    m = wod_e2ed_pb2.E2EDFrame()\n",
    "    m.ParseFromString(raw_bytes)\n",
    "    return m\n",
    "\n",
    "def split_context_name(ctx_name: str):\n",
    "    if not ctx_name or \"-\" not in ctx_name:\n",
    "        return ctx_name, None\n",
    "    base, suf = ctx_name.rsplit(\"-\", 1)\n",
    "    try:\n",
    "        return base, int(suf)\n",
    "    except ValueError:\n",
    "        return ctx_name, None\n",
    "\n",
    "def find_record_by_seg_and_idx(tfrecord_file, target_seg_id, target_key_idx, max_scan=None):\n",
    "    \"\"\"\n",
    "    åœ¨ä¸€ä¸ª tfrecord shard é‡Œæ‰¾æŸä¸ª segment çš„æŸä¸ª key_idx\n",
    "    æ‰¾åˆ°åˆ™è¿”å› e2eï¼Œå¦åˆ™è¿”å› None\n",
    "    \"\"\"\n",
    "    ds = tf.data.TFRecordDataset(\n",
    "        tfrecord_file,\n",
    "        compression_type=(\"GZIP\" if tfrecord_file.endswith(\".gz\") else \"\")\n",
    "    )\n",
    "    for i, r in enumerate(ds):\n",
    "        if max_scan is not None and i >= max_scan:\n",
    "            break\n",
    "        e2e = parse_e2ed(r.numpy())\n",
    "        ctx = e2e.frame.context.name\n",
    "        seg_id, key_idx = split_context_name(ctx)\n",
    "        if seg_id == target_seg_id and key_idx == target_key_idx:\n",
    "            return e2e\n",
    "    return None\n",
    "\n",
    "def _get_repeated(states_obj, field):\n",
    "    if states_obj is None or (not hasattr(states_obj, field)):\n",
    "        return None\n",
    "    return np.array(list(getattr(states_obj, field)), dtype=np.float32)\n",
    "\n",
    "def show_ego_status_one_record(e2e, head=5):\n",
    "    \"\"\"\n",
    "    è½»é‡å±•ç¤ºï¼šåªæ‰“å°ä½ æœ€å…³å¿ƒçš„ ego statusï¼ˆpast/futureï¼‰\n",
    "    \"\"\"\n",
    "    ctx = e2e.frame.context.name\n",
    "    seg_id, key_idx = split_context_name(ctx)\n",
    "    print(\"=== Keyframe ===\")\n",
    "    print(\"context.name:\", ctx)\n",
    "    print(\"segment_id:\", seg_id)\n",
    "    print(\"key_idx:\", key_idx)\n",
    "    print(\"intent:\", int(getattr(e2e, \"intent\", -1)))\n",
    "\n",
    "    past = getattr(e2e, \"past_states\", None)\n",
    "    fut  = getattr(e2e, \"future_states\", None)\n",
    "\n",
    "    # ä½ ç›®å‰ç‰ˆæœ¬é‡Œå¸¸è§å­—æ®µï¼ˆä¸ä¿è¯å…¨éƒ½æœ‰ï¼Œæ‰€ä»¥åšäº†å®¹é”™ï¼‰\n",
    "    past_fields = [\"pos_x\",\"pos_y\",\"vel_x\",\"vel_y\",\"accel_x\",\"accel_y\"]\n",
    "    fut_fields  = [\"pos_x\",\"pos_y\",\"pos_z\"]  # ä½ ä¹‹å‰çœ‹åˆ° future æœ‰ pos_z\n",
    "\n",
    "    print(\"\\n=== past_states ===\")\n",
    "    for f in past_fields:\n",
    "        arr = _get_repeated(past, f)\n",
    "        if arr is None:\n",
    "            continue\n",
    "        print(f\"{f}: len={len(arr)} head={arr[:head]} tail={arr[-head:]}\")\n",
    "\n",
    "    print(\"\\n=== future_states ===\")\n",
    "    for f in fut_fields:\n",
    "        arr = _get_repeated(fut, f)\n",
    "        if arr is None:\n",
    "            continue\n",
    "        print(f\"{f}: len={len(arr)} head={arr[:head]} tail={arr[-head:]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a132df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from waymo_open_dataset import dataset_pb2 as open_dataset\n",
    "from waymo_open_dataset.protos import end_to_end_driving_data_pb2 as wod_e2ed_pb2\n",
    "\n",
    "FILENAME = r\"/mnt/d/Datasets/WOD_E2E_Camera_v1/val/val_202504211843.tfrecord-00008-of-00093\"\n",
    "SEG_ID   = \"fb0ed944efebd34d756103188d59da85\"\n",
    "KEY_IDX  = 223   # ä¾‹å­ï¼šä½ ä¹‹å‰å‡ºç°è¿‡ 20~223\n",
    "\n",
    "e2e = find_record_by_seg_and_idx(FILENAME, SEG_ID, KEY_IDX, max_scan=None)\n",
    "print(\"found:\", e2e is not None)\n",
    "\n",
    "if e2e is not None:\n",
    "    show_ego_status_one_record(e2e, head=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930a9a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "KEY_IDX  = 20   # ä¾‹å­ï¼šä½ ä¹‹å‰å‡ºç°è¿‡ 20~223\n",
    "\n",
    "e2e = find_record_by_seg_and_idx(FILENAME, SEG_ID, KEY_IDX, max_scan=None)\n",
    "print(\"found:\", e2e is not None)\n",
    "\n",
    "if e2e is not None:\n",
    "    show_ego_status_one_record(e2e, head=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065e4f44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de37ec7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad19d537",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c883ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9eb5c19",
   "metadata": {},
   "source": [
    "æ–°ç‰ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fae089d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "839e6990",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from waymo_open_dataset.protos import end_to_end_driving_data_pb2 as wod_e2ed_pb2\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "import matplotlib\n",
    "import math\n",
    "import random\n",
    "\n",
    "# 1. è§£å†³ç»˜å›¾åç«¯é—®é¢˜\n",
    "matplotlib.rcParams.update(matplotlib.rcParamsDefault)\n",
    "matplotlib.use('TkAgg', force=True)\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94718016",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847fcc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_e2ed(raw_bytes: bytes):\n",
    "    m = wod_e2ed_pb2.E2EDFrame()\n",
    "    m.ParseFromString(raw_bytes)\n",
    "    return m\n",
    "\n",
    "def split_context_name(ctx_name: str):\n",
    "    if not ctx_name or \"-\" not in ctx_name:\n",
    "        return ctx_name, None\n",
    "    base, suf = ctx_name.rsplit(\"-\", 1)\n",
    "    try:\n",
    "        return base, int(suf)\n",
    "    except ValueError:\n",
    "        return ctx_name, None\n",
    "\n",
    "def find_record_by_seg_and_idx(tfrecord_file, target_seg_id, target_key_idx, max_scan=None):\n",
    "    \"\"\"å¼¹æ€§ç‰ˆæœ¬ï¼Œå‚è€ƒæ•…éšœæ¢å¤æ–‡çŒ®çš„éš”ç¦»åŸåˆ™\"\"\"\n",
    "    try:\n",
    "        ds = tf.data.TFRecordDataset(\n",
    "            tfrecord_file,\n",
    "            compression_type=(\"GZIP\" if tfrecord_file.endswith(\".gz\") else \"\")\n",
    "        )\n",
    "        \n",
    "        count = 0\n",
    "        for i, r in enumerate(ds):\n",
    "            if max_scan is not None and i >= max_scan:\n",
    "                print(f\"å·²è¾¾åˆ°æœ€å¤§æ‰«æé™åˆ¶ {max_scan}\")\n",
    "                break\n",
    "                \n",
    "            try:\n",
    "                e2e = parse_e2ed(r.numpy())\n",
    "                ctx = e2e.frame.context.name\n",
    "                seg_id, key_idx = split_context_name(ctx)\n",
    "                \n",
    "                if seg_id == target_seg_id and key_idx == target_key_idx:\n",
    "                    print(f\"âœ… åœ¨ç´¢å¼• {i} æ‰¾åˆ°ç›®æ ‡è®°å½•\")\n",
    "                    return e2e, i  # è¿”å›è®°å½•å’Œä½ç½®ç´¢å¼•\n",
    "                \n",
    "                count += 1\n",
    "                if count % 100 == 0:\n",
    "                    print(f\"å·²æ‰«æ {count} æ¡è®°å½•...\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ å¤„ç†è®°å½• {i} æ—¶å‡ºé”™: {str(e)}\")\n",
    "                continue\n",
    "                \n",
    "        print(f\"æ‰«æå®Œæˆï¼Œå…±å¤„ç† {count} æ¡è®°å½•\")\n",
    "        return None, None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ æ–‡ä»¶è¯»å–é”™è¯¯: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "def safe_getattr(obj, attr_name, default=None):\n",
    "    \"\"\"å®‰å…¨è·å–å±æ€§ï¼Œç¬¦åˆæ•…éšœå¼¹æ€§è®¾è®¡\"\"\"\n",
    "    try:\n",
    "        return getattr(obj, attr_name)\n",
    "    except Exception as e:\n",
    "        return default\n",
    "\n",
    "def get_field_info(obj, field_name):\n",
    "    \"\"\"å®‰å…¨è·å–å­—æ®µä¿¡æ¯ï¼Œå‚è€ƒè¿è¡Œæ—¶æ•…éšœé¢„æµ‹æ–‡çŒ®\"\"\"\n",
    "    try:\n",
    "        value = safe_getattr(obj, field_name)\n",
    "        if value is None:\n",
    "            return {\"exists\": False, \"type\": \"None\", \"sample\": None}\n",
    "        \n",
    "        # å¤„ç†é‡å¤å­—æ®µ\n",
    "        if hasattr(value, '__len__') and not isinstance(value, (str, bytes)):\n",
    "            field_type = \"repeated\"\n",
    "            length = len(value)\n",
    "            sample = None\n",
    "            if length > 0:\n",
    "                try:\n",
    "                    # å°è¯•è·å–ç¬¬ä¸€ä¸ªå…ƒç´ çš„ç±»å‹\n",
    "                    if hasattr(value[0], 'DESCRIPTOR'):\n",
    "                        sample_type = value[0].DESCRIPTOR.name\n",
    "                        sample = f\"<{sample_type}>\"\n",
    "                    else:\n",
    "                        sample = str(value[0])[:50]\n",
    "                except Exception as e:\n",
    "                    sample = f\"error getting sample: {str(e)}\"\n",
    "            return {\n",
    "                \"exists\": True,\n",
    "                \"type\": field_type,\n",
    "                \"length\": length,\n",
    "                \"sample\": sample\n",
    "            }\n",
    "        \n",
    "        # å¤„ç†åµŒå¥—æ¶ˆæ¯\n",
    "        elif hasattr(value, 'DESCRIPTOR'):\n",
    "            return {\n",
    "                \"exists\": True,\n",
    "                \"type\": \"message\",\n",
    "                \"message_type\": value.DESCRIPTOR.name,\n",
    "                \"fields\": list(value.DESCRIPTOR.fields_by_name.keys())[:5]  # åªå–å‰5ä¸ªå­—æ®µ\n",
    "            }\n",
    "        \n",
    "        # åŸºæœ¬ç±»å‹\n",
    "        else:\n",
    "            return {\n",
    "                \"exists\": True,\n",
    "                \"type\": type(value).__name__,\n",
    "                \"value\": str(value)[:100]  # é™åˆ¶é•¿åº¦\n",
    "            }\n",
    "            \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"exists\": False,\n",
    "            \"error\": str(e),\n",
    "            \"type\": \"error\"\n",
    "        }\n",
    "\n",
    "def inspect_e2e_fields(e2e):\n",
    "    \"\"\"\n",
    "    å¼¹æ€§å­—æ®µæ£€æŸ¥ - å‚è€ƒæ•…éšœæ¢å¤æ–‡çŒ®ä¸­çš„è¿è¡Œæ—¶æ¢ç´¢åŸåˆ™\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"ğŸ” å¼¹æ€§ç»“æ„æ¢ç´¢ - WOD-E2E æ•°æ®é›†åˆ†æ\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 1. é¦–å…ˆæ£€æŸ¥é¡¶å±‚ç»“æ„\n",
    "    print(\"\\n=== ğŸ—ï¸ 1. é¡¶å±‚ç»“æ„åˆ†æ ===\")\n",
    "    explore_protobuf(e2e, \"  \", max_depth=2)\n",
    "    \n",
    "    # 2. é‡ç‚¹æ£€æŸ¥äº”ä¸ªå…³é”®å­—æ®µï¼Œä½¿ç”¨å®‰å…¨è®¿é—®\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ğŸ¯ 2. äº”ä¸ªå…³é”®å­—æ®µè¯¦ç»†åˆ†æ\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    key_fields = ['frame', 'past_states', 'future_states', 'intent', 'preference_trajectories']\n",
    "    for field_name in key_fields:\n",
    "        print(f\"\\n--- {field_name.upper()} ---\")\n",
    "        field_value = safe_getattr(e2e, field_name)\n",
    "        if field_value is not None:\n",
    "            explore_protobuf(field_value, \"  \", max_depth=2)\n",
    "        else:\n",
    "            print(f\"  âš ï¸ å­—æ®µ '{field_name}' ä¸å­˜åœ¨æˆ–ä¸ºç©º\")\n",
    "    \n",
    "    # 3. Intent å­—æ®µæ·±åº¦åˆ†æ\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ§  3. INTENT å­—æ®µæ·±åº¦åˆ†æ (å‚è€ƒæ•…éšœé¢„æµ‹æ–‡çŒ®)\")\n",
    "    print(\"=\"*60)\n",
    "    intent_analysis = analyze_intent_field(e2e)\n",
    "    \n",
    "    # 4. Preference Trajectories æ·±åº¦åˆ†æ\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ›£ï¸ 4. PREFERENCE TRAJECTORIES æ·±åº¦åˆ†æ (å‚è€ƒæ•…éšœæ¢å¤æ–‡çŒ®)\")\n",
    "    print(\"=\"*60)\n",
    "    prefs_analysis = analyze_preference_trajectories(e2e)\n",
    "    \n",
    "    return intent_analysis, prefs_analysis\n",
    "\n",
    "def explore_protobuf(obj, prefix=\"\", depth=0, max_depth=3):\n",
    "    \"\"\"é€’å½’æ¢ç´¢protobufç»“æ„ï¼Œå®‰å…¨ç‰ˆæœ¬\"\"\"\n",
    "    if depth > max_depth or obj is None:\n",
    "        return\n",
    "    \n",
    "    if hasattr(obj, 'DESCRIPTOR'):\n",
    "        fields = list(obj.DESCRIPTOR.fields_by_name.keys())\n",
    "        print(f\"{prefix}ğŸ” {obj.DESCRIPTOR.name}, å­—æ®µæ•°: {len(fields)}\")\n",
    "        \n",
    "        # åªå±•ç¤ºå‰10ä¸ªå­—æ®µé¿å…è¾“å‡ºè¿‡é•¿\n",
    "        for field_name in fields[:10]:\n",
    "            field_info = get_field_info(obj, field_name)\n",
    "            if field_info[\"exists\"]:\n",
    "                if field_info[\"type\"] == \"message\":\n",
    "                    print(f\"{prefix}  ğŸ“¦ '{field_name}': {field_info['message_type']}\")\n",
    "                    if depth < max_depth - 1:  # é™åˆ¶é€’å½’æ·±åº¦\n",
    "                        nested_obj = safe_getattr(obj, field_name)\n",
    "                        if nested_obj:\n",
    "                            explore_protobuf(nested_obj, prefix + \"    \", depth + 1, max_depth)\n",
    "                elif field_info[\"type\"] == \"repeated\":\n",
    "                    print(f\"{prefix}  ğŸ“‹ '{field_name}': é‡å¤å­—æ®µ, é•¿åº¦={field_info.get('length', 0)}, ç¤ºä¾‹={field_info.get('sample', 'N/A')}\")\n",
    "                else:\n",
    "                    print(f\"{prefix}  ğŸ”¹ '{field_name}': {field_info['type']}, å€¼={field_info.get('value', 'N/A')}\")\n",
    "            else:\n",
    "                print(f\"{prefix}  âš ï¸ '{field_name}': å­—æ®µä¸å­˜åœ¨æˆ–è®¿é—®é”™è¯¯\")\n",
    "        \n",
    "        if len(fields) > 10:\n",
    "            print(f\"{prefix}  ... (è¿˜æœ‰ {len(fields)-10} ä¸ªå­—æ®µæœªæ˜¾ç¤º)\")\n",
    "    \n",
    "    elif hasattr(obj, '__len__') and not isinstance(obj, (str, bytes, np.ndarray)):\n",
    "        print(f\"{prefix}ğŸ“‹ å®¹å™¨ç±»å‹: {type(obj).__name__}, é•¿åº¦={len(obj)}\")\n",
    "        if len(obj) > 0:\n",
    "            try:\n",
    "                sample = str(obj[0])[:50]\n",
    "                print(f\"{prefix}  ç¤ºä¾‹å€¼: {sample}\")\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "def serialize_protobuf_info(obj, max_depth=2, current_depth=0):\n",
    "    \"\"\"å°†protobufç»“æ„è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„å­—å…¸ï¼Œç¬¦åˆæ•…éšœéš”ç¦»åŸåˆ™\"\"\"\n",
    "    if current_depth > max_depth or obj is None:\n",
    "        return \"depth_limit\"\n",
    "    \n",
    "    result = {}\n",
    "    \n",
    "    if hasattr(obj, 'DESCRIPTOR'):\n",
    "        result[\"message_type\"] = obj.DESCRIPTOR.name\n",
    "        result[\"fields\"] = {}\n",
    "        \n",
    "        # é™åˆ¶å­—æ®µæ•°é‡\n",
    "        field_names = list(obj.DESCRIPTOR.fields_by_name.keys())[:20]\n",
    "        \n",
    "        for field_name in field_names:\n",
    "            field_info = get_field_info(obj, field_name)\n",
    "            if field_info[\"exists\"]:\n",
    "                if field_info[\"type\"] == \"message\":\n",
    "                    nested_obj = safe_getattr(obj, field_name)\n",
    "                    result[\"fields\"][field_name] = {\n",
    "                        \"type\": \"nested_message\",\n",
    "                        \"message_type\": field_info[\"message_type\"],\n",
    "                        \"fields_count\": len(field_info[\"fields\"]) if \"fields\" in field_info else 0\n",
    "                    }\n",
    "                    # é€’å½’åºåˆ—åŒ–ï¼Œä½†é™åˆ¶æ·±åº¦\n",
    "                    if current_depth < max_depth - 1:\n",
    "                        nested_info = serialize_protobuf_info(nested_obj, max_depth, current_depth + 1)\n",
    "                        result[\"fields\"][field_name][\"details\"] = nested_info\n",
    "                elif field_info[\"type\"] == \"repeated\":\n",
    "                    result[\"fields\"][field_name] = {\n",
    "                        \"type\": \"repeated\",\n",
    "                        \"length\": field_info.get(\"length\", 0),\n",
    "                        \"sample\": field_info.get(\"sample\", None)\n",
    "                    }\n",
    "                else:\n",
    "                    result[\"fields\"][field_name] = {\n",
    "                        \"type\": field_info[\"type\"],\n",
    "                        \"value\": field_info.get(\"value\", None)\n",
    "                    }\n",
    "            else:\n",
    "                result[\"fields\"][field_name] = {\"type\": \"error\", \"error\": field_info.get(\"error\", \"unknown error\")}\n",
    "    \n",
    "    return result\n",
    "\n",
    "def scan_available_indices(tfrecord_file, target_seg_id, max_scan=500):\n",
    "    \"\"\"æ‰«æå¯ç”¨ç´¢å¼•ï¼Œå‚è€ƒç²¾ç¡®æ“ä½œæ–‡çŒ®ä¸­çš„ç³»ç»Ÿæ€§æ¢ç´¢\"\"\"\n",
    "    print(f\"\\nğŸ” æ‰«ææ–‡ä»¶ä»¥æŸ¥æ‰¾ segment '{target_seg_id}' çš„å¯ç”¨ç´¢å¼•...\")\n",
    "    try:\n",
    "        ds = tf.data.TFRecordDataset(\n",
    "            tfrecord_file,\n",
    "            compression_type=(\"GZIP\" if tfrecord_file.endswith(\".gz\") else \"\")\n",
    "        )\n",
    "        \n",
    "        found_indices = []\n",
    "        positions = []\n",
    "        for i, r in enumerate(ds):\n",
    "            if i >= max_scan:\n",
    "                break\n",
    "                \n",
    "            try:\n",
    "                e2e = parse_e2ed(r.numpy())\n",
    "                ctx = e2e.frame.context.name\n",
    "                seg_id, key_idx = split_context_name(ctx)\n",
    "                \n",
    "                if seg_id == target_seg_id and key_idx is not None:\n",
    "                    found_indices.append(key_idx)\n",
    "                    positions.append(i)\n",
    "                    print(f\"  âœ… æ‰¾åˆ°ç´¢å¼•: {key_idx} (æ–‡ä»¶ä½ç½®: {i})\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        print(f\"æ‰«æå®Œæˆï¼Œæ‰¾åˆ° {len(found_indices)} ä¸ªå¯ç”¨ç´¢å¼•\")\n",
    "        return found_indices, positions\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ æ‰«æè¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}\")\n",
    "        return [], []\n",
    "\n",
    "def interpret_intent(intent_value):\n",
    "    \"\"\"æ ¹æ®æ–‡çŒ®å’Œå¸¸è§æ¨¡å¼è§£é‡Š intent å€¼\"\"\"\n",
    "    # åŸºäº Failure Prediction at Runtime æ–‡çŒ®çš„æ„å›¾åˆ†ç±»\n",
    "    intent_mapping = {\n",
    "        0: \"æœªçŸ¥/é»˜è®¤æ„å›¾\",\n",
    "        1: \"ç›´è¡Œ (Lane Following)\",\n",
    "        2: \"å·¦å˜é“ (Left Lane Change)\",\n",
    "        3: \"å³å˜é“ (Right Lane Change)\",\n",
    "        4: \"å·¦è½¬ (Left Turn)\",\n",
    "        5: \"å³è½¬ (Right Turn)\",\n",
    "        6: \"åœè½¦ (Stopping)\",\n",
    "        7: \"ç´§æ€¥é¿è®© (Emergency Evasion)\",\n",
    "        8: \"æ±‡å…¥ (Merging)\",\n",
    "        9: \"è®©è¡Œ (Yielding)\",\n",
    "        10: \"è·¯å£ç­‰å¾… (Intersection Waiting)\",\n",
    "        11: \"è¡Œäººé¿è®© (Pedestrian Yielding)\",\n",
    "        12: \"éšœç¢ç‰©é¿è®© (Obstacle Avoidance)\"\n",
    "    }\n",
    "    \n",
    "    # åŸºäº Robot Failure Recovery Using VLMs æ–‡çŒ®çš„å®‰å…¨å…³é”®æ„å›¾\n",
    "    safety_critical_intents = [7, 8, 9, 11, 12]  # é«˜é£é™©æ„å›¾\n",
    "    \n",
    "    if intent_value in intent_mapping:\n",
    "        meaning = intent_mapping[intent_value]\n",
    "        if intent_value in safety_critical_intents:\n",
    "            meaning += \" âš ï¸ (é«˜é£é™©æ„å›¾)\"\n",
    "        return meaning\n",
    "    else:\n",
    "        return f\"æœªçŸ¥æ„å›¾ä»£ç : {intent_value} (éœ€è¦æ ¹æ®æ•°æ®é›†æ–‡æ¡£ç¡®è®¤)\"\n",
    "\n",
    "def predict_failure_risk_from_intent(intent_value):\n",
    "    \"\"\"åŸºäºæ„å›¾é¢„æµ‹æ•…éšœé£é™©ï¼Œå‚è€ƒ Failure Prediction at Runtime æ–‡çŒ®\"\"\"\n",
    "    # åŸºäºæ–‡çŒ®çš„ç®€åŒ–é£é™©æ¨¡å‹\n",
    "    risk_mapping = {\n",
    "        0: 0.10,  # æœªçŸ¥ - ä¸­ç­‰é£é™©\n",
    "        1: 0.05,  # ç›´è¡Œ - ä½é£é™©\n",
    "        2: 0.15,  # å·¦å˜é“ - ä¸­ç­‰é£é™©\n",
    "        3: 0.15,  # å³å˜é“ - ä¸­ç­‰é£é™©\n",
    "        4: 0.25,  # å·¦è½¬ - è¾ƒé«˜é£é™©\n",
    "        5: 0.25,  # å³è½¬ - è¾ƒé«˜é£é™©\n",
    "        6: 0.10,  # åœè½¦ - ä½é£é™©\n",
    "        7: 0.45,  # ç´§æ€¥é¿è®© - é«˜é£é™©\n",
    "        8: 0.35,  # æ±‡å…¥ - é«˜é£é™©\n",
    "        9: 0.30,  # è®©è¡Œ - è¾ƒé«˜é£é™©\n",
    "        10: 0.20, # è·¯å£ç­‰å¾… - ä¸­ç­‰é£é™©\n",
    "        11: 0.40, # è¡Œäººé¿è®© - é«˜é£é™©\n",
    "        12: 0.35  # éšœç¢ç‰©é¿è®© - é«˜é£é™©\n",
    "    }\n",
    "    \n",
    "    base_risk = risk_mapping.get(intent_value, 0.2)  # é»˜è®¤ä¸­ç­‰é£é™©\n",
    "    \n",
    "    # æ·»åŠ éšæœºæ‰°åŠ¨æ¨¡æ‹Ÿä¸ç¡®å®šæ€§ï¼Œå‚è€ƒ Generative Robot Policies æ–‡çŒ®\n",
    "    uncertainty = random.uniform(-0.05, 0.05)\n",
    "    final_risk = max(0.0, min(1.0, base_risk + uncertainty))\n",
    "    \n",
    "    return final_risk\n",
    "\n",
    "def analyze_intent_field(e2e):\n",
    "    \"\"\"ä¸“é—¨åˆ†æ intent å­—æ®µï¼Œå‚è€ƒ Failure Prediction at Runtime æ–‡çŒ®\"\"\"\n",
    "    intent = safe_getattr(e2e, 'intent')\n",
    "    \n",
    "    if intent is None:\n",
    "        print(\"  âš ï¸ Intent å­—æ®µä¸å­˜åœ¨\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"  ğŸ” Intent ç±»å‹: {type(intent).__name__}\")\n",
    "    print(f\"  ğŸ’¡ Intent å€¼: {intent}\")\n",
    "    \n",
    "    # å°è¯•ç†è§£ intent çš„å«ä¹‰\n",
    "    intent_meaning = interpret_intent(intent)\n",
    "    print(f\"  ğŸ“– Intent è¯­ä¹‰: {intent_meaning}\")\n",
    "    \n",
    "    # æ•…éšœé¢„æµ‹ç›¸å…³åˆ†æ\n",
    "    failure_risk = predict_failure_risk_from_intent(intent)\n",
    "    print(f\"  âš ï¸ æ•…éšœé£é™©é¢„æµ‹: {failure_risk:.2f} (åŸºäºæ„å›¾åˆ†æ)\")\n",
    "    \n",
    "    # å¯è§†åŒ–å»ºè®®\n",
    "    print(f\"  ğŸ’¡ å¯è§†åŒ–å»ºè®®: å¯åˆ›å»ºæ„å›¾åˆ†å¸ƒç›´æ–¹å›¾ï¼Œåˆ†æä¸åŒæ„å›¾ä¸‹çš„æ•…éšœç‡\")\n",
    "    \n",
    "    return {\n",
    "        \"value\": int(intent),\n",
    "        \"semantic_meaning\": intent_meaning,\n",
    "        \"failure_risk\": failure_risk\n",
    "    }\n",
    "\n",
    "def analyze_single_trajectory(traj, prefix=\"\"):\n",
    "    \"\"\"åˆ†æå•ä¸ªè½¨è¿¹\"\"\"\n",
    "    if traj is None:\n",
    "        print(f\"{prefix}âš ï¸ è½¨è¿¹ä¸ºç©º\")\n",
    "        return {\"error\": \"è½¨è¿¹ä¸ºç©º\"}\n",
    "    \n",
    "    analysis = {\n",
    "        \"fields\": {},\n",
    "        \"length\": 0,\n",
    "        \"safety_score\": 0.0\n",
    "    }\n",
    "    \n",
    "    # æ£€æŸ¥è½¨è¿¹å­—æ®µ\n",
    "    fields_to_check = ['pos_x', 'pos_y', 'pos_z', 'yaw', 'speed', 'acceleration', 'preference_score']\n",
    "    \n",
    "    for field in fields_to_check:\n",
    "        field_analysis = {\"exists\": False}\n",
    "        \n",
    "        if hasattr(traj, field):\n",
    "            field_analysis[\"exists\"] = True\n",
    "            try:\n",
    "                value = getattr(traj, field)\n",
    "                if hasattr(value, '__len__'):  # æ˜¯åˆ—è¡¨/æ•°ç»„\n",
    "                    field_analysis[\"type\"] = \"repeated\"\n",
    "                    field_analysis[\"length\"] = len(value)\n",
    "                    if len(value) > 0:\n",
    "                        try:\n",
    "                            field_analysis[\"sample\"] = list(value)[:3]\n",
    "                        except:\n",
    "                            field_analysis[\"sample\"] = \"æ— æ³•è½¬æ¢ä¸ºåˆ—è¡¨\"\n",
    "                else:  # å•ä¸ªå€¼\n",
    "                    field_analysis[\"type\"] = \"scalar\"\n",
    "                    field_analysis[\"value\"] = float(value) if isinstance(value, (int, float)) else str(value)\n",
    "            except Exception as e:\n",
    "                field_analysis[\"error\"] = str(e)\n",
    "        \n",
    "        analysis[\"fields\"][field] = field_analysis\n",
    "        \n",
    "        # æ‰“å°ç»“æœ\n",
    "        if field_analysis[\"exists\"]:\n",
    "            if \"type\" in field_analysis and field_analysis[\"type\"] == \"repeated\":\n",
    "                print(f\"{prefix}âœ… '{field}': é•¿åº¦={field_analysis.get('length', 0)}\")\n",
    "                if \"sample\" in field_analysis:\n",
    "                    print(f\"{prefix}   ç¤ºä¾‹: {field_analysis['sample']}\")\n",
    "            elif \"type\" in field_analysis and field_analysis[\"type\"] == \"scalar\":\n",
    "                print(f\"{prefix}âœ… '{field}': {field_analysis.get('value', 'N/A')}\")\n",
    "            else:\n",
    "                print(f\"{prefix}âœ… '{field}': å­˜åœ¨ä½†ç±»å‹æœªçŸ¥\")\n",
    "        else:\n",
    "            print(f\"{prefix}âŒ '{field}': å­—æ®µä¸å­˜åœ¨\")\n",
    "    \n",
    "    # è®¡ç®—è½¨è¿¹é•¿åº¦ï¼ˆåŸºäºä½ç½®æ•°æ®ï¼‰\n",
    "    if hasattr(traj, 'pos_x') and hasattr(traj, 'pos_y'):\n",
    "        try:\n",
    "            xs = list(traj.pos_x)\n",
    "            ys = list(traj.pos_y)\n",
    "            analysis[\"length\"] = len(xs)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # è®¡ç®—å®‰å…¨åˆ†æ•°\n",
    "    analysis[\"safety_score\"] = calculate_trajectory_safety(traj)\n",
    "    print(f\"{prefix}ğŸ›¡ï¸ å®‰å…¨åˆ†æ•°: {analysis['safety_score']:.2f}/1.0\")\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "def analyze_preference_trajectories(e2e):\n",
    "    \"\"\"ä¸“é—¨åˆ†æ preference_trajectories å­—æ®µï¼Œå‚è€ƒ Robot Failure Recovery æ–‡çŒ®\"\"\"\n",
    "    prefs = safe_getattr(e2e, 'preference_trajectories')\n",
    "    \n",
    "    if prefs is None:\n",
    "        print(\"  âš ï¸ Preference trajectories å­—æ®µä¸å­˜åœ¨\")\n",
    "        return None\n",
    "    \n",
    "    print(\"  ğŸ” Preference trajectories ç»“æ„åˆ†æ:\")\n",
    "    explore_protobuf(prefs, \"    \", max_depth=2)\n",
    "    \n",
    "    # æ£€æŸ¥è½¨è¿¹æ•°é‡\n",
    "    trajectories = safe_getattr(prefs, 'trajectories')\n",
    "    if trajectories is None or not hasattr(trajectories, '__len__'):\n",
    "        print(\"  âš ï¸ æ— æ³•è®¿é—®è½¨è¿¹åˆ—è¡¨\")\n",
    "        return None\n",
    "    \n",
    "    num_trajectories = len(trajectories)\n",
    "    print(f\"  ğŸ“Š è½¨è¿¹æ•°é‡: {num_trajectories}\")\n",
    "    \n",
    "    trajectory_analyses = []\n",
    "    \n",
    "    if num_trajectories > 0:\n",
    "        print(f\"  ğŸ¯ é¦–é€‰è½¨è¿¹åˆ†æ (ç´¢å¼• 0):\")\n",
    "        primary_analysis = analyze_single_trajectory(trajectories[0], \"      \")\n",
    "        trajectory_analyses.append(primary_analysis)\n",
    "        \n",
    "        # å¦‚æœæœ‰å¤šä¸ªè½¨è¿¹ï¼Œåˆ†æå¤‡é€‰è½¨è¿¹\n",
    "        if num_trajectories > 1:\n",
    "            print(f\"  ğŸ”„ å¤‡é€‰è½¨è¿¹åˆ†æ (ç´¢å¼• 1-{min(num_trajectories-1, 3)}):\")\n",
    "            for i in range(1, min(num_trajectories, 4)):  # åªåˆ†æå‰3ä¸ªå¤‡é€‰\n",
    "                print(f\"    ğŸ›£ï¸ è½¨è¿¹ {i}:\")\n",
    "                backup_analysis = analyze_single_trajectory(trajectories[i], \"        \")\n",
    "                trajectory_analyses.append(backup_analysis)\n",
    "    \n",
    "    # æ•…éšœæ¢å¤åˆ†æ\n",
    "    recovery_analysis = analyze_trajectories_for_recovery(trajectories)\n",
    "    print(\"\\n  ğŸ›¡ï¸ æ•…éšœæ¢å¤èƒ½åŠ›åˆ†æ (å‚è€ƒ Vision-Language Models æ–‡çŒ®):\")\n",
    "    print(f\"    {recovery_analysis}\")\n",
    "    \n",
    "    # ç”Ÿæˆå»ºè®®\n",
    "    suggestions = generate_recovery_suggestions(trajectories)\n",
    "    \n",
    "    return {\n",
    "        \"num_trajectories\": num_trajectories,\n",
    "        \"trajectories_analysis\": trajectory_analyses,\n",
    "        \"recovery_analysis\": recovery_analysis,\n",
    "        \"suggestions\": suggestions\n",
    "    }\n",
    "\n",
    "def calculate_trajectory_safety(traj):\n",
    "    \"\"\"è®¡ç®—è½¨è¿¹å®‰å…¨æ€§ï¼Œå‚è€ƒ Precise and Dexterous Manipulation æ–‡çŒ®\"\"\"\n",
    "    # åŸºç¡€å®‰å…¨åˆ†æ•°\n",
    "    safety_score = 1.0\n",
    "    \n",
    "    # ä½ç½®å¹³æ»‘æ€§ï¼ˆæ›²ç‡ï¼‰\n",
    "    if hasattr(traj, 'pos_x') and hasattr(traj, 'pos_y'):\n",
    "        try:\n",
    "            xs = list(traj.pos_x)\n",
    "            ys = list(traj.pos_y)\n",
    "            \n",
    "            if len(xs) > 2:\n",
    "                # è®¡ç®—æ›²ç‡\n",
    "                curvatures = []\n",
    "                for i in range(1, len(xs)-1):\n",
    "                    x1, y1 = xs[i-1], ys[i-1]\n",
    "                    x2, y2 = xs[i], ys[i]  \n",
    "                    x3, y3 = xs[i+1], ys[i+1]\n",
    "                    \n",
    "                    # å‘é‡\n",
    "                    v1 = (x2-x1, y2-y1)\n",
    "                    v2 = (x3-x2, y3-y2)\n",
    "                    \n",
    "                    # æ›²ç‡è¿‘ä¼¼\n",
    "                    cross = v1[0]*v2[1] - v1[1]*v2[0]\n",
    "                    mag1 = math.sqrt(v1[0]**2 + v1[1]**2)\n",
    "                    mag2 = math.sqrt(v2[0]**2 + v2[1]**2)\n",
    "                    \n",
    "                    if mag1 > 0.1 and mag2 > 0.1:  # é¿å…é™¤é›¶\n",
    "                        curvature = abs(cross) / (mag1 * mag2)\n",
    "                        curvatures.append(curvature)\n",
    "                \n",
    "                if curvatures:\n",
    "                    max_curvature = max(curvatures)\n",
    "                    avg_curvature = sum(curvatures) / len(curvatures)\n",
    "                    # é«˜æ›²ç‡é™ä½å®‰å…¨æ€§\n",
    "                    safety_score *= max(0.1, 1.0 - max_curvature * 15)\n",
    "                    safety_score *= max(0.1, 1.0 - avg_curvature * 10)\n",
    "        except Exception as e:\n",
    "            pass\n",
    "    \n",
    "    # é€Ÿåº¦å¹³æ»‘æ€§\n",
    "    if hasattr(traj, 'speed'):\n",
    "        try:\n",
    "            speeds = list(traj.speed)\n",
    "            if len(speeds) > 1:\n",
    "                # è®¡ç®—é€Ÿåº¦å˜åŒ–ç‡ï¼ˆåŠ é€Ÿåº¦ï¼‰\n",
    "                accels = [abs(speeds[i+1] - speeds[i]) for i in range(len(speeds)-1)]\n",
    "                max_accel = max(accels) if accels else 0\n",
    "                # é«˜åŠ é€Ÿåº¦é™ä½å®‰å…¨æ€§\n",
    "                safety_score *= max(0.2, 1.0 - max_accel * 0.5)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return max(0.0, min(1.0, safety_score))\n",
    "\n",
    "def calculate_trajectory_diversity(trajectories):\n",
    "    \"\"\"è®¡ç®—è½¨è¿¹å¤šæ ·æ€§ï¼Œå‚è€ƒ Failure Resilience æ–‡çŒ®\"\"\"\n",
    "    if len(trajectories) < 2:\n",
    "        return 0.0\n",
    "    \n",
    "    # è®¡ç®—ç»ˆç‚¹ä½ç½®çš„åˆ†æ•£ç¨‹åº¦\n",
    "    endpoints = []\n",
    "    for traj in trajectories:\n",
    "        if hasattr(traj, 'pos_x') and hasattr(traj, 'pos_y'):\n",
    "            try:\n",
    "                xs = list(traj.pos_x)\n",
    "                ys = list(traj.pos_y)\n",
    "                if xs and ys and len(xs) > 0 and len(ys) > 0:\n",
    "                    endpoints.append((xs[-1], ys[-1]))\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    if len(endpoints) < 2:\n",
    "        return 0.0\n",
    "    \n",
    "    # è®¡ç®—å¹³å‡æˆå¯¹è·ç¦»\n",
    "    total_dist = 0\n",
    "    count = 0\n",
    "    for i in range(len(endpoints)):\n",
    "        for j in range(i+1, len(endpoints)):\n",
    "            dx = endpoints[i][0] - endpoints[j][0]\n",
    "            dy = endpoints[i][1] - endpoints[j][1]\n",
    "            dist = math.sqrt(dx*dx + dy*dy)\n",
    "            total_dist += dist\n",
    "            count += 1\n",
    "    \n",
    "    avg_dist = total_dist / count if count > 0 else 0\n",
    "    # å½’ä¸€åŒ–åˆ°0-1èŒƒå›´ (å‡è®¾100ç±³ä¸ºæœ€å¤§æœ‰æ„ä¹‰è·ç¦»)\n",
    "    return min(1.0, avg_dist / 100.0)\n",
    "\n",
    "def analyze_trajectories_for_recovery(trajectories):\n",
    "    \"\"\"åˆ†æè½¨è¿¹é›†åˆçš„æ•…éšœæ¢å¤èƒ½åŠ›\"\"\"\n",
    "    if not trajectories:\n",
    "        return \"æ— è½¨è¿¹å¯ç”¨äºåˆ†æ\"\n",
    "    \n",
    "    # å‚è€ƒ Robot Failure Recovery Using VLMs æ–‡çŒ®\n",
    "    diversity_score = calculate_trajectory_diversity(trajectories)\n",
    "    safety_scores = [calculate_trajectory_safety(traj) for traj in trajectories]\n",
    "    \n",
    "    analysis = []\n",
    "    analysis.append(f\"è½¨è¿¹å¤šæ ·æ€§å¾—åˆ†: {diversity_score:.2f}/1.0\")\n",
    "    analysis.append(f\"å®‰å…¨åˆ†æ•°èŒƒå›´: [{min(safety_scores):.2f}, {max(safety_scores):.2f}]\")\n",
    "    analysis.append(f\"é¦–é€‰è½¨è¿¹å®‰å…¨åˆ†: {safety_scores[0]:.2f}\")\n",
    "    \n",
    "    if len(trajectories) > 1:\n",
    "        backup_safety = max(safety_scores[1:])  # æœ€ä½³å¤‡é€‰\n",
    "        analysis.append(f\"æœ€ä½³å¤‡é€‰å®‰å…¨åˆ†: {backup_safety:.2f}\")\n",
    "        if backup_safety > safety_scores[0] * 0.8:  # å¤‡é€‰è‡³å°‘æœ‰é¦–é€‰80%çš„å®‰å…¨æ€§\n",
    "            analysis.append(\"âœ… æœ‰é«˜è´¨é‡å¤‡é€‰è½¨è¿¹ï¼Œæ•…éšœæ¢å¤èƒ½åŠ›å¼º\")\n",
    "        else:\n",
    "            analysis.append(\"âš ï¸ å¤‡é€‰è½¨è¿¹è´¨é‡è¾ƒä½ï¼Œæ•…éšœæ¢å¤èƒ½åŠ›æœ‰é™\")\n",
    "    \n",
    "    return \"\\n    \".join(analysis)\n",
    "\n",
    "def generate_recovery_suggestions(trajectories):\n",
    "    \"\"\"ç”Ÿæˆæ•…éšœæ¢å¤å»ºè®®ï¼Œå‚è€ƒ Vision-Language Models æ–‡çŒ®\"\"\"\n",
    "    print(\"\\n  ğŸ’¡ ç”Ÿæˆå¼æ•…éšœæ¢å¤å»ºè®®:\")\n",
    "    \n",
    "    num_trajectories = len(trajectories)\n",
    "    if num_trajectories == 0:\n",
    "        print(\"    âŒ æ— å¯ç”¨è½¨è¿¹ï¼Œå»ºè®®è¯·æ±‚è¿œç¨‹æ“ä½œå‘˜å¹²é¢„\")\n",
    "        return [\"æ— å¯ç”¨è½¨è¿¹\"]\n",
    "    \n",
    "    # é¦–é€‰è½¨è¿¹åˆ†æ\n",
    "    primary_traj = trajectories[0]\n",
    "    primary_safety = calculate_trajectory_safety(primary_traj)\n",
    "    \n",
    "    suggestions = []\n",
    "    \n",
    "    if primary_safety < 0.3:\n",
    "        suggestions.append(\"ğŸš¨ é¦–é€‰è½¨è¿¹å®‰å…¨æ€§æä½ï¼Œå»ºè®®ç«‹å³åˆ‡æ¢åˆ°å¤‡é€‰è½¨è¿¹\")\n",
    "    elif primary_safety < 0.6:\n",
    "        suggestions.append(\"âš ï¸ é¦–é€‰è½¨è¿¹å®‰å…¨æ€§ä¸­ç­‰ï¼Œå»ºè®®ç›‘æ§å¹¶å‡†å¤‡åˆ‡æ¢\")\n",
    "    else:\n",
    "        suggestions.append(\"âœ… é¦–é€‰è½¨è¿¹å®‰å…¨æ€§é«˜ï¼Œå¯ç»§ç»­æ‰§è¡Œ\")\n",
    "    \n",
    "    # å¤‡é€‰è½¨è¿¹åˆ†æ\n",
    "    if num_trajectories > 1:\n",
    "        backup_safeties = [calculate_trajectory_safety(traj) for traj in trajectories[1:]]\n",
    "        best_backup = max(backup_safeties)\n",
    "        \n",
    "        if best_backup > 0.7:\n",
    "            suggestions.append(f\"ğŸ›¡ï¸ æœ‰é«˜è´¨é‡å¤‡é€‰è½¨è¿¹ (å®‰å…¨åˆ†: {best_backup:.2f})ï¼Œæ•…éšœæ¢å¤èƒ½åŠ›å¼º\")\n",
    "        elif best_backup > 0.4:\n",
    "            suggestions.append(f\"ğŸ”„ å¤‡é€‰è½¨è¿¹è´¨é‡ä¸€èˆ¬ (å®‰å…¨åˆ†: {best_backup:.2f})ï¼Œéœ€è¦è°¨æ…åˆ‡æ¢\")\n",
    "        else:\n",
    "            suggestions.append(f\"â— å¤‡é€‰è½¨è¿¹è´¨é‡ä½ (æœ€é«˜å®‰å…¨åˆ†: {best_backup:.2f})ï¼Œå»ºè®®è¯·æ±‚äººç±»å¹²é¢„\")\n",
    "        \n",
    "        # è½¨è¿¹å¤šæ ·æ€§\n",
    "        diversity = calculate_trajectory_diversity(trajectories)\n",
    "        if diversity > 0.6:\n",
    "            suggestions.append(\"ğŸŒˆ è½¨è¿¹å¤šæ ·æ€§é«˜ï¼Œç³»ç»Ÿèƒ½é€‚åº”å¤šç§åœºæ™¯å˜åŒ–\")\n",
    "        elif diversity > 0.3:\n",
    "            suggestions.append(\"ğŸŸ¡ è½¨è¿¹å¤šæ ·æ€§ä¸­ç­‰ï¼Œè¦†ç›–éƒ¨åˆ†å¼‚å¸¸æƒ…å†µ\")\n",
    "        else:\n",
    "            suggestions.append(\"ğŸ”´ è½¨è¿¹å¤šæ ·æ€§ä½ï¼Œç³»ç»Ÿå¼¹æ€§æœ‰é™ï¼Œå»ºè®®å¢åŠ è½¨è¿¹ç”Ÿæˆç­–ç•¥\")\n",
    "    else:\n",
    "        suggestions.append(\"ğŸš« æ— å¤‡é€‰è½¨è¿¹ï¼Œå•ç‚¹æ•…éšœé£é™©é«˜ï¼Œå»ºè®®å¢åŠ è½¨è¿¹å¤šæ ·æ€§\")\n",
    "    \n",
    "    # ç”Ÿæˆäººç±»å¯è¯»å»ºè®®\n",
    "    for i, suggestion in enumerate(suggestions, 1):\n",
    "        print(f\"    {i}. {suggestion}\")\n",
    "    \n",
    "    # ç»“åˆæ–‡çŒ®çš„é«˜çº§å»ºè®®\n",
    "    print(\"\\n  ğŸ“š åŸºäºæœ€æ–°ç ”ç©¶çš„é«˜çº§å»ºè®®:\")\n",
    "    print(\"    â€¢ å‚è€ƒ 'Robot Failure Recovery Using VLMs'ï¼šé›†æˆè§†è§‰-è¯­è¨€æ¨¡å‹è¿›è¡Œå®æ—¶è½¨è¿¹è¯„ä¼°\")\n",
    "    print(\"    â€¢ å‚è€ƒ 'Failure Prediction at Runtime'ï¼šåœ¨è½¨è¿¹æ‰§è¡Œå‰é¢„æµ‹æ•…éšœæ¦‚ç‡\")\n",
    "    print(\"    â€¢ å‚è€ƒ 'Precise and Dexterous Manipulation'ï¼šä½¿ç”¨äººç±»åé¦ˆä¼˜åŒ–è½¨è¿¹ç”Ÿæˆ\")\n",
    "    \n",
    "    return suggestions\n",
    "\n",
    "def save_analysis_results(SEG_ID, KEY_IDX, e2e, file_position, intent_analysis=None, prefs_analysis=None):\n",
    "    \"\"\"å®‰å…¨ä¿å­˜åˆ†æç»“æœï¼Œå‚è€ƒæ•…éšœæ¢å¤æ–‡çŒ®çš„æŒä¹…åŒ–åŸåˆ™\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_dir = \"e2e_analysis_results\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # 1. ä¿å­˜ç»“æ„ä¿¡æ¯ï¼ˆå¯åºåˆ—åŒ–ç‰ˆæœ¬ï¼‰\n",
    "    structure_info = {\n",
    "        \"segment_id\": SEG_ID,\n",
    "        \"key_idx\": KEY_IDX,\n",
    "        \"file_position\": file_position,\n",
    "        \"timestamp\": timestamp,\n",
    "        \"top_level_fields\": list(e2e.DESCRIPTOR.fields_by_name.keys()),\n",
    "        \"detailed_structure\": serialize_protobuf_info(e2e, max_depth=2)\n",
    "    }\n",
    "    \n",
    "    # æ·»åŠ intentå’Œpreferenceåˆ†æ\n",
    "    if intent_analysis is not None:\n",
    "        structure_info[\"intent_analysis\"] = intent_analysis\n",
    "    \n",
    "    if prefs_analysis is not None:\n",
    "        structure_info[\"preference_trajectories_analysis\"] = prefs_analysis\n",
    "    \n",
    "    json_file = os.path.join(output_dir, f\"structure_{SEG_ID}_{KEY_IDX}_{timestamp}.json\")\n",
    "    with open(json_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(structure_info, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"âœ… ç»“æ„ä¿¡æ¯å·²ä¿å­˜åˆ°: {json_file}\")\n",
    "    \n",
    "    # 2. ä¿å­˜å…³é”®æ•°æ®æ‘˜è¦\n",
    "    summary = {\n",
    "        \"segment_id\": SEG_ID,\n",
    "        \"key_idx\": KEY_IDX,\n",
    "        \"file_position\": file_position,\n",
    "        \"context_name\": safe_getattr(e2e.frame, 'context', {}).name if hasattr(e2e, 'frame') else \"N/A\",\n",
    "        \"timestamp_micros\": safe_getattr(e2e.frame, 'timestamp_micros', \"N/A\") if hasattr(e2e, 'frame') else \"N/A\"\n",
    "    }\n",
    "    \n",
    "    # æ·»åŠ  past_states ä¿¡æ¯\n",
    "    if hasattr(e2e, 'past_states'):\n",
    "        past = e2e.past_states\n",
    "        summary[\"past_states\"] = {\n",
    "            \"available_fields\": [f for f in ['pos_x', 'pos_y', 'vel_x', 'vel_y'] if hasattr(past, f)],\n",
    "            \"sequence_lengths\": {}\n",
    "        }\n",
    "        for field in ['pos_x', 'pos_y', 'vel_x', 'vel_y']:\n",
    "            if hasattr(past, field):\n",
    "                try:\n",
    "                    arr = list(getattr(past, field))\n",
    "                    summary[\"past_states\"][\"sequence_lengths\"][field] = len(arr)\n",
    "                    if len(arr) > 0:\n",
    "                        summary[\"past_states\"][f\"{field}_sample\"] = arr[:3]\n",
    "                except:\n",
    "                    pass\n",
    "    \n",
    "    # æ·»åŠ  future_states ä¿¡æ¯\n",
    "    if hasattr(e2e, 'future_states'):\n",
    "        future = e2e.future_states\n",
    "        summary[\"future_states\"] = {\n",
    "            \"available_fields\": [f for f in ['pos_x', 'pos_y', 'pos_z'] if hasattr(future, f)],\n",
    "            \"sequence_lengths\": {}\n",
    "        }\n",
    "        for field in ['pos_x', 'pos_y', 'pos_z']:\n",
    "            if hasattr(future, field):\n",
    "                try:\n",
    "                    arr = list(getattr(future, field))\n",
    "                    summary[\"future_states\"][\"sequence_lengths\"][field] = len(arr)\n",
    "                    if len(arr) > 0:\n",
    "                        summary[\"future_states\"][f\"{field}_sample\"] = arr[:3]\n",
    "                except:\n",
    "                    pass\n",
    "    \n",
    "    # æ·»åŠ intentä¿¡æ¯\n",
    "    if intent_analysis is not None:\n",
    "        summary[\"intent\"] = intent_analysis\n",
    "    \n",
    "    # æ·»åŠ preference trajectoriesæ‘˜è¦\n",
    "    if prefs_analysis is not None:\n",
    "        summary[\"preference_trajectories\"] = {\n",
    "            \"num_trajectories\": prefs_analysis[\"num_trajectories\"],\n",
    "            \"avg_safety_score\": sum(t[\"safety_score\"] for t in prefs_analysis[\"trajectories_analysis\"] if \"safety_score\" in t) / len(prefs_analysis[\"trajectories_analysis\"]) if prefs_analysis[\"trajectories_analysis\"] else 0,\n",
    "            \"recovery_capability\": \"é«˜\" if prefs_analysis[\"recovery_analysis\"] and \"âœ…\" in prefs_analysis[\"recovery_analysis\"] else \"ä¸­/ä½\"\n",
    "        }\n",
    "    \n",
    "    summary_file = os.path.join(output_dir, f\"summary_{SEG_ID}_{KEY_IDX}_{timestamp}.json\")\n",
    "    with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(summary, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"âœ… æ•°æ®æ‘˜è¦å·²ä¿å­˜åˆ°: {summary_file}\")\n",
    "    \n",
    "    # 3. ç”Ÿæˆäººç±»å¯è¯»çš„æŠ¥å‘Š\n",
    "    report = f\"\"\"\n",
    "WOD-E2E æ•°æ®é›†åˆ†ææŠ¥å‘Š\n",
    "=======================\n",
    "ç”Ÿæˆæ—¶é—´: {timestamp}\n",
    "Segment ID: {SEG_ID}\n",
    "Key Index: {KEY_IDX}\n",
    "æ–‡ä»¶ä½ç½®: {file_position}\n",
    "\n",
    "é¡¶å±‚å­—æ®µ:\n",
    "{', '.join(e2e.DESCRIPTOR.fields_by_name.keys())}\n",
    "\n",
    "å…³é”®å‘ç°:\n",
    "\"\"\"\n",
    "    \n",
    "    if hasattr(e2e, 'past_states'):\n",
    "        past = e2e.past_states\n",
    "        report += f\"\\nå†å²çŠ¶æ€å­—æ®µ:\\n\"\n",
    "        for field in ['pos_x', 'pos_y', 'vel_x', 'vel_y']:\n",
    "            if hasattr(past, field):\n",
    "                try:\n",
    "                    arr = list(getattr(past, field))\n",
    "                    report += f\"  - {field}: é•¿åº¦={len(arr)}, ç¤ºä¾‹={arr[:3]}\\n\"\n",
    "                except:\n",
    "                    report += f\"  - {field}: å­˜åœ¨ä½†è®¿é—®å¤±è´¥\\n\"\n",
    "    \n",
    "    # æ·»åŠ intentåˆ†æ\n",
    "    if intent_analysis is not None:\n",
    "        report += f\"\\næ„å›¾åˆ†æ:\\n\"\n",
    "        report += f\"  - æ„å›¾å€¼: {intent_analysis['value']}\\n\"\n",
    "        report += f\"  - è¯­ä¹‰å«ä¹‰: {intent_analysis['semantic_meaning']}\\n\"\n",
    "        report += f\"  - æ•…éšœé£é™©: {intent_analysis['failure_risk']:.2f}\\n\"\n",
    "    \n",
    "    # æ·»åŠ preferenceè½¨è¿¹åˆ†æ\n",
    "    if prefs_analysis is not None:\n",
    "        report += f\"\\nåå¥½è½¨è¿¹åˆ†æ:\\n\"\n",
    "        report += f\"  - è½¨è¿¹æ•°é‡: {prefs_analysis['num_trajectories']}\\n\"\n",
    "        report += f\"  - æ•…éšœæ¢å¤èƒ½åŠ›: \\n{prefs_analysis['recovery_analysis']}\\n\"\n",
    "        report += \"\\n  - æ¢å¤å»ºè®®:\\n\"\n",
    "        for i, suggestion in enumerate(prefs_analysis['suggestions'], 1):\n",
    "            report += f\"    {i}. {suggestion}\\n\"\n",
    "    \n",
    "    report_file = os.path.join(output_dir, f\"report_{SEG_ID}_{KEY_IDX}_{timestamp}.txt\")\n",
    "    with open(report_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(report)\n",
    "    print(f\"âœ… äººç±»å¯è¯»æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}\")\n",
    "    \n",
    "    return output_dir\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec14b664",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd5d7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ å¯åŠ¨ WOD-E2E æ•°æ®é›†å¼¹æ€§åˆ†æå·¥å…·\n",
      "ğŸ“ æ–‡ä»¶: /mnt/d/Datasets/WOD_E2E_Camera_v1/val/val_202504211843.tfrecord-00008-of-00093\n",
      "ğŸ¯ ç›®æ ‡ Segment ID: fb0ed944efebd34d756103188d59da85\n",
      "\n",
      "================================================================================\n",
      "1ï¸âƒ£ ç¬¬ä¸€æ­¥ï¼šå¼¹æ€§æ‰«æå¯ç”¨ç´¢å¼•ï¼ˆå‚è€ƒæ•…éšœæ¢å¤æ–‡çŒ®ï¼‰\n",
      "================================================================================\n",
      "\n",
      "ğŸ” æ‰«ææ–‡ä»¶ä»¥æŸ¥æ‰¾ segment 'fb0ed944efebd34d756103188d59da85' çš„å¯ç”¨ç´¢å¼•...\n",
      "  âœ… æ‰¾åˆ°ç´¢å¼•: 223 (æ–‡ä»¶ä½ç½®: 78)\n",
      "æ‰«æå®Œæˆï¼Œæ‰¾åˆ° 1 ä¸ªå¯ç”¨ç´¢å¼•\n",
      "\n",
      "âœ… æ‰¾åˆ° 1 ä¸ªå¯ç”¨ç´¢å¼•: [223]\n",
      "ğŸ¯ é€‰æ‹©ç¬¬ä¸€ä¸ªç´¢å¼•: 223 (æ–‡ä»¶ä½ç½®: 78)\n",
      "\n",
      "================================================================================\n",
      "2ï¸âƒ£ ç¬¬äºŒæ­¥ï¼šæŸ¥æ‰¾ segment 'fb0ed944efebd34d756103188d59da85' ç´¢å¼• 223 çš„è®°å½•\n",
      "================================================================================\n",
      "âœ… åœ¨ç´¢å¼• 78 æ‰¾åˆ°ç›®æ ‡è®°å½•\n",
      "\n",
      "âœ… æˆåŠŸæ‰¾åˆ°è®°å½•! (å®é™…æ–‡ä»¶ä½ç½®: 78)\n",
      "\n",
      "================================================================================\n",
      "3ï¸âƒ£ ç¬¬ä¸‰æ­¥ï¼šå®‰å…¨ç»“æ„åˆ†æï¼ˆé¿å…ä¸å¯åºåˆ—åŒ–å¯¹è±¡ï¼‰\n",
      "================================================================================\n",
      "============================================================\n",
      "ğŸ” å¼¹æ€§ç»“æ„æ¢ç´¢ - WOD-E2E æ•°æ®é›†åˆ†æ\n",
      "============================================================\n",
      "\n",
      "=== ğŸ—ï¸ 1. é¡¶å±‚ç»“æ„åˆ†æ ===\n",
      "  ğŸ” E2EDFrame, å­—æ®µæ•°: 5\n",
      "    ğŸ“¦ 'frame': Frame\n",
      "      ğŸ” Frame, å­—æ®µæ•°: 11\n",
      "        ğŸ“¦ 'context': Context\n",
      "        ğŸ”¹ 'timestamp_micros': int, å€¼=0\n",
      "        ğŸ“¦ 'pose': Transform\n",
      "        ğŸ“‹ 'images': é‡å¤å­—æ®µ, é•¿åº¦=8, ç¤ºä¾‹=<CameraImage>\n",
      "        ğŸ“‹ 'lasers': é‡å¤å­—æ®µ, é•¿åº¦=0, ç¤ºä¾‹=None\n",
      "        ğŸ“‹ 'laser_labels': é‡å¤å­—æ®µ, é•¿åº¦=0, ç¤ºä¾‹=None\n",
      "        ğŸ“‹ 'projected_lidar_labels': é‡å¤å­—æ®µ, é•¿åº¦=0, ç¤ºä¾‹=None\n",
      "        ğŸ“‹ 'camera_labels': é‡å¤å­—æ®µ, é•¿åº¦=0, ç¤ºä¾‹=None\n",
      "        ğŸ“‹ 'no_label_zones': é‡å¤å­—æ®µ, é•¿åº¦=0, ç¤ºä¾‹=None\n",
      "        ğŸ“‹ 'map_features': é‡å¤å­—æ®µ, é•¿åº¦=0, ç¤ºä¾‹=None\n",
      "        ... (è¿˜æœ‰ 1 ä¸ªå­—æ®µæœªæ˜¾ç¤º)\n",
      "    ğŸ“¦ 'future_states': EgoTrajectoryStates\n",
      "      ğŸ” EgoTrajectoryStates, å­—æ®µæ•°: 8\n",
      "        ğŸ“‹ 'pos_x': é‡å¤å­—æ®µ, é•¿åº¦=20, ç¤ºä¾‹=0.8994140625\n",
      "        ğŸ“‹ 'pos_y': é‡å¤å­—æ®µ, é•¿åº¦=20, ç¤ºä¾‹=0.005859375\n",
      "        ğŸ“‹ 'pos_z': é‡å¤å­—æ®µ, é•¿åº¦=20, ç¤ºä¾‹=0.01024627685546875\n",
      "        ğŸ“‹ 'vel_x': é‡å¤å­—æ®µ, é•¿åº¦=0, ç¤ºä¾‹=None\n",
      "        ğŸ“‹ 'vel_y': é‡å¤å­—æ®µ, é•¿åº¦=0, ç¤ºä¾‹=None\n",
      "        ğŸ“‹ 'accel_x': é‡å¤å­—æ®µ, é•¿åº¦=0, ç¤ºä¾‹=None\n",
      "        ğŸ“‹ 'accel_y': é‡å¤å­—æ®µ, é•¿åº¦=0, ç¤ºä¾‹=None\n",
      "        ğŸ”¹ 'preference_score': float, å€¼=0.0\n",
      "    ğŸ“¦ 'past_states': EgoTrajectoryStates\n",
      "      ğŸ” EgoTrajectoryStates, å­—æ®µæ•°: 8\n",
      "        ğŸ“‹ 'pos_x': é‡å¤å­—æ®µ, é•¿åº¦=16, ç¤ºä¾‹=-21.7568359375\n",
      "        ğŸ“‹ 'pos_y': é‡å¤å­—æ®µ, é•¿åº¦=16, ç¤ºä¾‹=-0.02978515625\n",
      "        ğŸ“‹ 'pos_z': é‡å¤å­—æ®µ, é•¿åº¦=0, ç¤ºä¾‹=None\n",
      "        ğŸ“‹ 'vel_x': é‡å¤å­—æ®µ, é•¿åº¦=16, ç¤ºä¾‹=6.426236152648926\n",
      "        ğŸ“‹ 'vel_y': é‡å¤å­—æ®µ, é•¿åº¦=16, ç¤ºä¾‹=0.10726787894964218\n",
      "        ğŸ“‹ 'accel_x': é‡å¤å­—æ®µ, é•¿åº¦=16, ç¤ºä¾‹=0.20577478408813477\n",
      "        ğŸ“‹ 'accel_y': é‡å¤å­—æ®µ, é•¿åº¦=16, ç¤ºä¾‹=-0.02171137183904648\n",
      "        ğŸ”¹ 'preference_score': float, å€¼=0.0\n",
      "    ğŸ”¹ 'intent': int, å€¼=1\n",
      "    ğŸ“‹ 'preference_trajectories': é‡å¤å­—æ®µ, é•¿åº¦=3, ç¤ºä¾‹=<EgoTrajectoryStates>\n",
      "\n",
      "==================================================\n",
      "ğŸ¯ 2. äº”ä¸ªå…³é”®å­—æ®µè¯¦ç»†åˆ†æ\n",
      "==================================================\n",
      "\n",
      "--- FRAME ---\n",
      "  ğŸ” Frame, å­—æ®µæ•°: 11\n",
      "    ğŸ“¦ 'context': Context\n",
      "      ğŸ” Context, å­—æ®µæ•°: 4\n",
      "        ğŸ”¹ 'name': str, å€¼=fb0ed944efebd34d756103188d59da85-223\n",
      "        ğŸ“‹ 'camera_calibrations': é‡å¤å­—æ®µ, é•¿åº¦=8, ç¤ºä¾‹=<CameraCalibration>\n",
      "        ğŸ“‹ 'laser_calibrations': é‡å¤å­—æ®µ, é•¿åº¦=0, ç¤ºä¾‹=None\n",
      "        ğŸ“¦ 'stats': Stats\n",
      "    ğŸ”¹ 'timestamp_micros': int, å€¼=0\n",
      "    ğŸ“¦ 'pose': Transform\n",
      "      ğŸ” Transform, å­—æ®µæ•°: 1\n",
      "        ğŸ“‹ 'transform': é‡å¤å­—æ®µ, é•¿åº¦=0, ç¤ºä¾‹=None\n",
      "    ğŸ“‹ 'images': é‡å¤å­—æ®µ, é•¿åº¦=8, ç¤ºä¾‹=<CameraImage>\n",
      "    ğŸ“‹ 'lasers': é‡å¤å­—æ®µ, é•¿åº¦=0, ç¤ºä¾‹=None\n",
      "    ğŸ“‹ 'laser_labels': é‡å¤å­—æ®µ, é•¿åº¦=0, ç¤ºä¾‹=None\n",
      "    ğŸ“‹ 'projected_lidar_labels': é‡å¤å­—æ®µ, é•¿åº¦=0, ç¤ºä¾‹=None\n",
      "    ğŸ“‹ 'camera_labels': é‡å¤å­—æ®µ, é•¿åº¦=0, ç¤ºä¾‹=None\n",
      "    ğŸ“‹ 'no_label_zones': é‡å¤å­—æ®µ, é•¿åº¦=0, ç¤ºä¾‹=None\n",
      "    ğŸ“‹ 'map_features': é‡å¤å­—æ®µ, é•¿åº¦=0, ç¤ºä¾‹=None\n",
      "    ... (è¿˜æœ‰ 1 ä¸ªå­—æ®µæœªæ˜¾ç¤º)\n",
      "\n",
      "--- PAST_STATES ---\n",
      "  ğŸ” EgoTrajectoryStates, å­—æ®µæ•°: 8\n",
      "    ğŸ“‹ 'pos_x': é‡å¤å­—æ®µ, é•¿åº¦=16, ç¤ºä¾‹=-21.7568359375\n",
      "    ğŸ“‹ 'pos_y': é‡å¤å­—æ®µ, é•¿åº¦=16, ç¤ºä¾‹=-0.02978515625\n",
      "    ğŸ“‹ 'pos_z': é‡å¤å­—æ®µ, é•¿åº¦=0, ç¤ºä¾‹=None\n",
      "    ğŸ“‹ 'vel_x': é‡å¤å­—æ®µ, é•¿åº¦=16, ç¤ºä¾‹=6.426236152648926\n",
      "    ğŸ“‹ 'vel_y': é‡å¤å­—æ®µ, é•¿åº¦=16, ç¤ºä¾‹=0.10726787894964218\n",
      "    ğŸ“‹ 'accel_x': é‡å¤å­—æ®µ, é•¿åº¦=16, ç¤ºä¾‹=0.20577478408813477\n",
      "    ğŸ“‹ 'accel_y': é‡å¤å­—æ®µ, é•¿åº¦=16, ç¤ºä¾‹=-0.02171137183904648\n",
      "    ğŸ”¹ 'preference_score': float, å€¼=0.0\n",
      "\n",
      "--- FUTURE_STATES ---\n",
      "  ğŸ” EgoTrajectoryStates, å­—æ®µæ•°: 8\n",
      "    ğŸ“‹ 'pos_x': é‡å¤å­—æ®µ, é•¿åº¦=20, ç¤ºä¾‹=0.8994140625\n",
      "    ğŸ“‹ 'pos_y': é‡å¤å­—æ®µ, é•¿åº¦=20, ç¤ºä¾‹=0.005859375\n",
      "    ğŸ“‹ 'pos_z': é‡å¤å­—æ®µ, é•¿åº¦=20, ç¤ºä¾‹=0.01024627685546875\n",
      "    ğŸ“‹ 'vel_x': é‡å¤å­—æ®µ, é•¿åº¦=0, ç¤ºä¾‹=None\n",
      "    ğŸ“‹ 'vel_y': é‡å¤å­—æ®µ, é•¿åº¦=0, ç¤ºä¾‹=None\n",
      "    ğŸ“‹ 'accel_x': é‡å¤å­—æ®µ, é•¿åº¦=0, ç¤ºä¾‹=None\n",
      "    ğŸ“‹ 'accel_y': é‡å¤å­—æ®µ, é•¿åº¦=0, ç¤ºä¾‹=None\n",
      "    ğŸ”¹ 'preference_score': float, å€¼=0.0\n",
      "\n",
      "--- INTENT ---\n",
      "\n",
      "--- PREFERENCE_TRAJECTORIES ---\n",
      "  ğŸ“‹ å®¹å™¨ç±»å‹: RepeatedCompositeContainer, é•¿åº¦=3\n",
      "    ç¤ºä¾‹å€¼: preference_score: -1.0\n",
      "\n",
      "\n",
      "============================================================\n",
      "ğŸ§  3. INTENT å­—æ®µæ·±åº¦åˆ†æ (å‚è€ƒæ•…éšœé¢„æµ‹æ–‡çŒ®)\n",
      "============================================================\n",
      "  ğŸ” Intent ç±»å‹: int\n",
      "  ğŸ’¡ Intent å€¼: 1\n",
      "  ğŸ“– Intent è¯­ä¹‰: ç›´è¡Œ (Lane Following)\n",
      "  âš ï¸ æ•…éšœé£é™©é¢„æµ‹: 0.00 (åŸºäºæ„å›¾åˆ†æ)\n",
      "  ğŸ’¡ å¯è§†åŒ–å»ºè®®: å¯åˆ›å»ºæ„å›¾åˆ†å¸ƒç›´æ–¹å›¾ï¼Œåˆ†æä¸åŒæ„å›¾ä¸‹çš„æ•…éšœç‡\n",
      "\n",
      "============================================================\n",
      "ğŸ›£ï¸ 4. PREFERENCE TRAJECTORIES æ·±åº¦åˆ†æ (å‚è€ƒæ•…éšœæ¢å¤æ–‡çŒ®)\n",
      "============================================================\n",
      "  âš ï¸ æ— æ³•è®¿é—®è½¨è¿¹åˆ—è¡¨\n",
      "\n",
      "================================================================================\n",
      "4ï¸âƒ£ ç¬¬å››æ­¥ï¼šå¼¹æ€§ä¿å­˜åˆ†æç»“æœï¼ˆå‚è€ƒæ•…éšœé¢„æµ‹æ–‡çŒ®ï¼‰\n",
      "================================================================================\n",
      "âœ… ç»“æ„ä¿¡æ¯å·²ä¿å­˜åˆ°: e2e_analysis_results/structure_fb0ed944efebd34d756103188d59da85_223_20260107_150946.json\n",
      "âœ… æ•°æ®æ‘˜è¦å·²ä¿å­˜åˆ°: e2e_analysis_results/summary_fb0ed944efebd34d756103188d59da85_223_20260107_150946.json\n",
      "âœ… äººç±»å¯è¯»æŠ¥å‘Šå·²ä¿å­˜åˆ°: e2e_analysis_results/report_fb0ed944efebd34d756103188d59da85_223_20260107_150946.txt\n",
      "\n",
      "ğŸ‰ åˆ†æå®Œæˆï¼æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: e2e_analysis_results\n",
      "\n",
      "ğŸ’¡ åç»­å»ºè®®:\n",
      "1. æŸ¥çœ‹ç”Ÿæˆçš„ JSON æ–‡ä»¶äº†è§£è¯¦ç»†ç»“æ„\n",
      "2. æ£€æŸ¥ report_*.txt æ–‡ä»¶è·å–äººç±»å¯è¯»æ‘˜è¦\n",
      "3. æ ¹æ®åˆ†æç»“æœè°ƒæ•´ä½ çš„ç«¯åˆ°ç«¯é©¾é©¶æ¨¡å‹è¾“å…¥\n",
      "ğŸ“ˆ è½¨è¿¹å¯è§†åŒ–å·²ä¿å­˜åˆ°: e2e_analysis_results/trajectory_analysis_fb0ed944efebd34d756103188d59da85_223.png\n",
      "\n",
      "================================================================================\n",
      "3ï¸âƒ£ ç¬¬ä¸‰æ­¥ï¼šPreference Trajectories ä¸“é¡¹åˆ†æ\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "ğŸ›£ï¸ PREFERENCE TRAJECTORIES åˆ†æ\n",
      "============================================================\n",
      "  âš ï¸ æ— æ³•è®¿é—®è½¨è¿¹åˆ—è¡¨\n",
      "âœ… ç»“æ„ä¿¡æ¯å·²ä¿å­˜åˆ°: e2e_analysis_results/structure_fb0ed944efebd34d756103188d59da85_223_20260107_150947.json\n",
      "âœ… æ•°æ®æ‘˜è¦å·²ä¿å­˜åˆ°: e2e_analysis_results/summary_fb0ed944efebd34d756103188d59da85_223_20260107_150947.json\n",
      "âœ… äººç±»å¯è¯»æŠ¥å‘Šå·²ä¿å­˜åˆ°: e2e_analysis_results/report_fb0ed944efebd34d756103188d59da85_223_20260107_150947.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_27870/566089376.py:115: UserWarning: Glyph 20301 (\\N{CJK UNIFIED IDEOGRAPH-4F4D}) missing from current font.\n",
      "  plt.savefig(viz_file)\n",
      "/tmp/ipykernel_27870/566089376.py:115: UserWarning: Glyph 32622 (\\N{CJK UNIFIED IDEOGRAPH-7F6E}) missing from current font.\n",
      "  plt.savefig(viz_file)\n",
      "/tmp/ipykernel_27870/566089376.py:115: UserWarning: Glyph 36712 (\\N{CJK UNIFIED IDEOGRAPH-8F68}) missing from current font.\n",
      "  plt.savefig(viz_file)\n",
      "/tmp/ipykernel_27870/566089376.py:115: UserWarning: Glyph 36857 (\\N{CJK UNIFIED IDEOGRAPH-8FF9}) missing from current font.\n",
      "  plt.savefig(viz_file)\n",
      "/tmp/ipykernel_27870/566089376.py:115: UserWarning: Glyph 35268 (\\N{CJK UNIFIED IDEOGRAPH-89C4}) missing from current font.\n",
      "  plt.savefig(viz_file)\n",
      "/tmp/ipykernel_27870/566089376.py:115: UserWarning: Glyph 21010 (\\N{CJK UNIFIED IDEOGRAPH-5212}) missing from current font.\n",
      "  plt.savefig(viz_file)\n",
      "/tmp/ipykernel_27870/566089376.py:115: UserWarning: Glyph 20998 (\\N{CJK UNIFIED IDEOGRAPH-5206}) missing from current font.\n",
      "  plt.savefig(viz_file)\n",
      "/tmp/ipykernel_27870/566089376.py:115: UserWarning: Glyph 26512 (\\N{CJK UNIFIED IDEOGRAPH-6790}) missing from current font.\n",
      "  plt.savefig(viz_file)\n",
      "/tmp/ipykernel_27870/566089376.py:115: UserWarning: Glyph 24403 (\\N{CJK UNIFIED IDEOGRAPH-5F53}) missing from current font.\n",
      "  plt.savefig(viz_file)\n",
      "/tmp/ipykernel_27870/566089376.py:115: UserWarning: Glyph 21069 (\\N{CJK UNIFIED IDEOGRAPH-524D}) missing from current font.\n",
      "  plt.savefig(viz_file)\n",
      "/tmp/ipykernel_27870/566089376.py:115: UserWarning: Glyph 24847 (\\N{CJK UNIFIED IDEOGRAPH-610F}) missing from current font.\n",
      "  plt.savefig(viz_file)\n",
      "/tmp/ipykernel_27870/566089376.py:115: UserWarning: Glyph 22270 (\\N{CJK UNIFIED IDEOGRAPH-56FE}) missing from current font.\n",
      "  plt.savefig(viz_file)\n",
      "/tmp/ipykernel_27870/566089376.py:115: UserWarning: Glyph 30452 (\\N{CJK UNIFIED IDEOGRAPH-76F4}) missing from current font.\n",
      "  plt.savefig(viz_file)\n",
      "/tmp/ipykernel_27870/566089376.py:115: UserWarning: Glyph 34892 (\\N{CJK UNIFIED IDEOGRAPH-884C}) missing from current font.\n",
      "  plt.savefig(viz_file)\n",
      "/tmp/ipykernel_27870/566089376.py:115: UserWarning: Glyph 25925 (\\N{CJK UNIFIED IDEOGRAPH-6545}) missing from current font.\n",
      "  plt.savefig(viz_file)\n",
      "/tmp/ipykernel_27870/566089376.py:115: UserWarning: Glyph 38556 (\\N{CJK UNIFIED IDEOGRAPH-969C}) missing from current font.\n",
      "  plt.savefig(viz_file)\n",
      "/tmp/ipykernel_27870/566089376.py:115: UserWarning: Glyph 39118 (\\N{CJK UNIFIED IDEOGRAPH-98CE}) missing from current font.\n",
      "  plt.savefig(viz_file)\n",
      "/tmp/ipykernel_27870/566089376.py:115: UserWarning: Glyph 38505 (\\N{CJK UNIFIED IDEOGRAPH-9669}) missing from current font.\n",
      "  plt.savefig(viz_file)\n",
      "/tmp/ipykernel_27870/566089376.py:115: UserWarning: Glyph 21382 (\\N{CJK UNIFIED IDEOGRAPH-5386}) missing from current font.\n",
      "  plt.savefig(viz_file)\n",
      "/tmp/ipykernel_27870/566089376.py:115: UserWarning: Glyph 21490 (\\N{CJK UNIFIED IDEOGRAPH-53F2}) missing from current font.\n",
      "  plt.savefig(viz_file)\n",
      "/tmp/ipykernel_27870/566089376.py:115: UserWarning: Glyph 39318 (\\N{CJK UNIFIED IDEOGRAPH-9996}) missing from current font.\n",
      "  plt.savefig(viz_file)\n",
      "/tmp/ipykernel_27870/566089376.py:115: UserWarning: Glyph 36873 (\\N{CJK UNIFIED IDEOGRAPH-9009}) missing from current font.\n",
      "  plt.savefig(viz_file)\n",
      "/tmp/ipykernel_27870/566089376.py:115: UserWarning: Glyph 26410 (\\N{CJK UNIFIED IDEOGRAPH-672A}) missing from current font.\n",
      "  plt.savefig(viz_file)\n",
      "/tmp/ipykernel_27870/566089376.py:115: UserWarning: Glyph 26469 (\\N{CJK UNIFIED IDEOGRAPH-6765}) missing from current font.\n",
      "  plt.savefig(viz_file)\n",
      "/tmp/ipykernel_27870/566089376.py:115: UserWarning: Glyph 30446 (\\N{CJK UNIFIED IDEOGRAPH-76EE}) missing from current font.\n",
      "  plt.savefig(viz_file)\n",
      "/tmp/ipykernel_27870/566089376.py:115: UserWarning: Glyph 26631 (\\N{CJK UNIFIED IDEOGRAPH-6807}) missing from current font.\n",
      "  plt.savefig(viz_file)\n"
     ]
    }
   ],
   "source": [
    "# ä¸»æ‰§è¡Œæµç¨‹ - å®Œå…¨å¼¹æ€§ç‰ˆæœ¬\n",
    "FILENAME = r\"/mnt/d/Datasets/WOD_E2E_Camera_v1/val/val_202504211843.tfrecord-00008-of-00093\"\n",
    "SEG_ID   = \"fb0ed944efebd34d756103188d59da85\"\n",
    "\n",
    "print(\"ğŸš€ å¯åŠ¨ WOD-E2E æ•°æ®é›†å¼¹æ€§åˆ†æå·¥å…·\")\n",
    "print(f\"ğŸ“ æ–‡ä»¶: {FILENAME}\")\n",
    "print(f\"ğŸ¯ ç›®æ ‡ Segment ID: {SEG_ID}\")\n",
    "\n",
    "# ç¬¬ä¸€æ­¥ï¼šæ‰«æå¯ç”¨ç´¢å¼•\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"1ï¸âƒ£ ç¬¬ä¸€æ­¥ï¼šå¼¹æ€§æ‰«æå¯ç”¨ç´¢å¼•ï¼ˆå‚è€ƒæ•…éšœæ¢å¤æ–‡çŒ®ï¼‰\")\n",
    "print(\"=\"*80)\n",
    "available_indices, positions = scan_available_indices(FILENAME, SEG_ID, max_scan=300)\n",
    "\n",
    "if not available_indices:\n",
    "    print(\"âŒ æœªæ‰¾åˆ°ä»»ä½•å¯ç”¨ç´¢å¼•ã€‚è¯·å°è¯•:\")\n",
    "    print(\"1. æ£€æŸ¥ segment ID æ˜¯å¦æ­£ç¡®\")\n",
    "    print(\"2. æ‰©å¤§æ‰«æèŒƒå›´ (max_scan)\")\n",
    "    print(\"3. æ£€æŸ¥æ–‡ä»¶è·¯å¾„å’Œæƒé™\")\n",
    "    exit(1)\n",
    "\n",
    "print(f\"\\nâœ… æ‰¾åˆ° {len(available_indices)} ä¸ªå¯ç”¨ç´¢å¼•: {available_indices}\")\n",
    "KEY_IDX = available_indices[0]\n",
    "file_position = positions[0]\n",
    "print(f\"ğŸ¯ é€‰æ‹©ç¬¬ä¸€ä¸ªç´¢å¼•: {KEY_IDX} (æ–‡ä»¶ä½ç½®: {file_position})\")\n",
    "\n",
    "# ç¬¬äºŒæ­¥ï¼šæŸ¥æ‰¾è®°å½•\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"2ï¸âƒ£ ç¬¬äºŒæ­¥ï¼šæŸ¥æ‰¾ segment '{SEG_ID}' ç´¢å¼• {KEY_IDX} çš„è®°å½•\")\n",
    "print(\"=\"*80)\n",
    "e2e, actual_position = find_record_by_seg_and_idx(FILENAME, SEG_ID, KEY_IDX, max_scan=500)\n",
    "\n",
    "if e2e is not None:\n",
    "    print(f\"\\nâœ… æˆåŠŸæ‰¾åˆ°è®°å½•! (å®é™…æ–‡ä»¶ä½ç½®: {actual_position})\")\n",
    "    \n",
    "    # ç¬¬ä¸‰æ­¥ï¼šç»“æ„åˆ†æ\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"3ï¸âƒ£ ç¬¬ä¸‰æ­¥ï¼šå®‰å…¨ç»“æ„åˆ†æï¼ˆé¿å…ä¸å¯åºåˆ—åŒ–å¯¹è±¡ï¼‰\")\n",
    "    print(\"=\"*80)\n",
    "    intent_analysis, prefs_analysis = inspect_e2e_fields(e2e)\n",
    "    \n",
    "    # ç¬¬å››æ­¥ï¼šå®‰å…¨ä¿å­˜ç»“æœ\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"4ï¸âƒ£ ç¬¬å››æ­¥ï¼šå¼¹æ€§ä¿å­˜åˆ†æç»“æœï¼ˆå‚è€ƒæ•…éšœé¢„æµ‹æ–‡çŒ®ï¼‰\")\n",
    "    print(\"=\"*80)\n",
    "    output_dir = save_analysis_results(SEG_ID, KEY_IDX, e2e, actual_position, intent_analysis, prefs_analysis)\n",
    "    \n",
    "    print(f\"\\nğŸ‰ åˆ†æå®Œæˆï¼æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {output_dir}\")\n",
    "    print(\"\\nğŸ’¡ åç»­å»ºè®®:\")\n",
    "    print(\"1. æŸ¥çœ‹ç”Ÿæˆçš„ JSON æ–‡ä»¶äº†è§£è¯¦ç»†ç»“æ„\")\n",
    "    print(\"2. æ£€æŸ¥ report_*.txt æ–‡ä»¶è·å–äººç±»å¯è¯»æ‘˜è¦\")\n",
    "    print(\"3. æ ¹æ®åˆ†æç»“æœè°ƒæ•´ä½ çš„ç«¯åˆ°ç«¯é©¾é©¶æ¨¡å‹è¾“å…¥\")\n",
    "    \n",
    "    # ç¬¬äº”æ­¥ï¼šç”Ÿæˆè½¨è¿¹å¯è§†åŒ–\n",
    "    try:\n",
    "        if hasattr(e2e, 'past_states') and hasattr(e2e.past_states, 'pos_x'):\n",
    "            past_x = list(e2e.past_states.pos_x)\n",
    "            past_y = list(e2e.past_states.pos_y)\n",
    "            \n",
    "            # é¦–é€‰è½¨è¿¹\n",
    "            if hasattr(e2e, 'future_states') and hasattr(e2e.future_states, 'pos_x'):\n",
    "                future_x = list(e2e.future_states.pos_x)\n",
    "                future_y = list(e2e.future_states.pos_y)\n",
    "                \n",
    "                # åˆ›å»ºè½¨è¿¹å›¾\n",
    "                plt.figure(figsize=(12, 10))\n",
    "                \n",
    "                # å†å²è½¨è¿¹\n",
    "                plt.plot(past_x, past_y, 'b-', linewidth=2, label='å†å²è½¨è¿¹', alpha=0.8)\n",
    "                plt.scatter(past_x[-1], past_y[-1], c='blue', s=100, marker='o', label='å½“å‰ä½ç½®')\n",
    "                \n",
    "                # æœªæ¥è½¨è¿¹\n",
    "                plt.plot(future_x, future_y, 'r-', linewidth=2, label='é¦–é€‰æœªæ¥è½¨è¿¹', alpha=0.8)\n",
    "                plt.scatter(future_x[-1], future_y[-1], c='red', s=100, marker='x', label='ç›®æ ‡ä½ç½®')\n",
    "                \n",
    "                # åˆ†æ preference trajectories\n",
    "                if hasattr(e2e, 'preference_trajectories') and hasattr(e2e.preference_trajectories, 'trajectories'):\n",
    "                    prefs = e2e.preference_trajectories\n",
    "                    trajectories = prefs.trajectories\n",
    "                    \n",
    "                    # åªç»˜åˆ¶å‰3ä¸ªå¤‡é€‰è½¨è¿¹\n",
    "                    for i in range(1, min(len(trajectories), 4)):\n",
    "                        try:\n",
    "                            traj = trajectories[i]\n",
    "                            if hasattr(traj, 'pos_x') and hasattr(traj, 'pos_y'):\n",
    "                                alt_x = list(traj.pos_x)\n",
    "                                alt_y = list(traj.pos_y)\n",
    "                                \n",
    "                                # è®¡ç®—å®‰å…¨åˆ†æ•°ç”¨äºé¢œè‰²å’Œé€æ˜åº¦\n",
    "                                safety = calculate_trajectory_safety(traj)\n",
    "                                color = plt.cm.viridis(safety)  # ä½¿ç”¨ colormap æ ¹æ®å®‰å…¨åˆ†æ•°æ˜ å°„é¢œè‰²\n",
    "                                alpha = 0.3 + 0.7 * safety  # å®‰å…¨æ€§è¶Šé«˜ï¼Œè¶Šä¸é€æ˜\n",
    "                                \n",
    "                                label = f'å¤‡é€‰è½¨è¿¹ {i} (å®‰å…¨åˆ†: {safety:.2f})'\n",
    "                                plt.plot(alt_x, alt_y, color=color, linewidth=1.5, alpha=alpha, label=label)\n",
    "                        except Exception as e:\n",
    "                            continue\n",
    "                \n",
    "                # æ·»åŠ æ„å›¾ä¿¡æ¯\n",
    "                if intent_analysis is not None:\n",
    "                    plt.text(0.05, 0.95, f\"å½“å‰æ„å›¾: {intent_analysis['semantic_meaning']}\\n\" +\n",
    "                            f\"æ•…éšœé£é™©: {intent_analysis['failure_risk']:.2f}\", \n",
    "                            transform=plt.gca().transAxes, fontsize=10,\n",
    "                            bbox=dict(facecolor='yellow', alpha=0.3, edgecolor='none'))\n",
    "                \n",
    "                plt.title(f'è½¨è¿¹è§„åˆ’åˆ†æ - Segment: {SEG_ID}, Index: {KEY_IDX}')\n",
    "                plt.xlabel('X ä½ç½® (m)')\n",
    "                plt.ylabel('Y ä½ç½® (m)')\n",
    "                plt.legend(loc='best')\n",
    "                plt.grid(True)\n",
    "                plt.axis('equal')\n",
    "                \n",
    "                \n",
    "                viz_file = os.path.join(output_dir, f\"trajectory_analysis_{SEG_ID}_{KEY_IDX}.png\")\n",
    "                plt.savefig(viz_file)\n",
    "                plt.close()\n",
    "                print(f\"ğŸ“ˆ è½¨è¿¹å¯è§†åŒ–å·²ä¿å­˜åˆ°: {viz_file}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ ç”Ÿæˆå¯è§†åŒ–æ—¶å‡ºé”™ (å¯å¿½ç•¥): {str(e)}\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ æœªèƒ½æ‰¾åˆ°è®°å½•ã€‚å¼¹æ€§æ¢å¤å»ºè®®:\")\n",
    "    print(\"1. å°è¯•å…¶ä»–å¯ç”¨ç´¢å¼•:\", available_indices[1:])\n",
    "    print(\"2. æ£€æŸ¥ Waymo Open Dataset ç‰ˆæœ¬æ˜¯å¦åŒ¹é…\")\n",
    "    print(\"3. éªŒè¯ protobuf å®šä¹‰æ–‡ä»¶æ˜¯å¦ä¸ºæœ€æ–°ç‰ˆæœ¬\")\n",
    "    print(\"4. æ ¹æ®æ•…éšœæ¢å¤æ–‡çŒ®ï¼Œè€ƒè™‘ä½¿ç”¨å¤‡ç”¨æ•°æ®æº\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dacf770",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25252788",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64ecf40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "waymo_clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
